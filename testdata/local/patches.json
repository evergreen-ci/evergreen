{"_id":{"$oid":"5ecedafb562343215a7ff297"},"desc":"dist","branch":"evergreen","githash":"33016573166a36bd5f46b4111151899d5c4e95b1","patch_number":1251,"author":"admin","version":"5ecedafb562343215a7ff297","status":"success","parameters":[{"key":"this-is-a-parameter","value":"yes-it-is"},{"key":"my_team","value":"sdc_buggy"}],"create_time":{"$date":"2020-05-27T21:26:19.434Z"},"start_time":{"$date":"2020-05-27T21:42:25.742Z"},"finish_time":{"$date":"2020-05-27T21:54:43.323Z"},"build_variants":["ubuntu1604"],"tasks":["dist"],"variants_tasks":[{"variant":"ubuntu1604","tasks":["dist"],"displaytasks":null}],"sync_at_end_opts":{"timeout":3600000000000},"patches":[{"name":"","githash":"33016573166a36bd5f46b4111151899d5c4e95b1","patch_set":{"patch_file_id":"5ecedafb562343215a7ff296","summary":[{"filename":"public/static/js/spawned_hosts.js","additions":36,"deletions":13},{"filename":"service/spawn.go","additions":4,"deletions":0},{"filename":"service/templates/admin.html","additions":1,"deletions":1},{"filename":"service/templates/spawned_hosts.html","additions":1,"deletions":1}]},"message":"dist"}],"activated":true,"alias":""}
{"_id":{"$oid":"6ecedafb562343215a7ff297"},"desc":"dist","branch":"evergreen","githash":"33016573166a36bd5f46b4111151899d5c4e95b1","patch_number":1251,"author":"bob.hicks","version":"6ecedafb562343215a7ff297","status":"success","create_time":{"$date":"2020-05-27T21:26:19.434Z"},"start_time":{"$date":"2020-05-27T21:42:25.742Z"},"finish_time":{"$date":"2020-05-27T21:54:43.323Z"},"build_variants":["ubuntu1604"],"tasks":["dist"],"variants_tasks":[{"variant":"ubuntu1604","tasks":["dist"],"displaytasks":null}],"sync_at_end_opts":{"timeout":3600000000000},"patches":[{"name":"","githash":"33016573166a36bd5f46b4111151899d5c4e95b1","patch_set":{"patch_file_id":"5ecedafb562343215a7ff296","summary":[{"filename":"public/static/js/spawned_hosts.js","additions":36,"deletions":13},{"filename":"service/spawn.go","additions":4,"deletions":0},{"filename":"service/templates/admin.html","additions":1,"deletions":1},{"filename":"service/templates/spawned_hosts.html","additions":1,"deletions":1}]},"message":"dist"}],"activated":true,"alias":""}
{"_id":{"$oid":"5ee1efb3d1fe073e194e8b5c"},"desc":"Commit Queue Merge: 'Server-48053 Fix transient_txn_error_labels_with_write_concern.js for multiversion testing' into 'mongodb/mongo:main'","branch":"mongodb-mongo-master","githash":"0af9c85d7e2ba60f592f2d7a9a35217e254e59fb","patch_number":96,"author":"pierlauro.sciarelli","version":"5ee1efb3d1fe073e194e8b5c","status":"started","create_time":{"$date":"2020-06-11T08:47:48.6Z"},"start_time":{"$date":"2020-06-11T08:49:06.349Z"},"finish_time":{"$date":{"$numberLong":"-62135596800000"}},"build_variants":["enterprise-rhel-62-64-bit","commit-queue","commit-queue-merge"],"tasks":["lint_pylinters","dbtest","lint_yaml","merge-patch","lint_clang_format","lint_cpplint","lint_errorcodes","lint_fuzzer_sanity_patch","lint_eslint","commit_queue_placeholder","validate_commit_message","compile_core_tools"],"variants_tasks":[{"variant":"enterprise-rhel-62-64-bit","tasks":["lint_clang_format","lint_eslint","lint_cpplint","lint_errorcodes","lint_fuzzer_sanity_patch","lint_pylinters","dbtest","lint_yaml"],"displaytasks":null},{"variant":"commit-queue","tasks":["commit_queue_placeholder","validate_commit_message","compile_core_tools"],"displaytasks":null},{"variant":"commit-queue-merge","tasks":["merge-patch"],"displaytasks":null}],"patches":[{"name":"","githash":"0af9c85d7e2ba60f592f2d7a9a35217e254e59fb","patch_set":{"patch_file_id":"5ee1efb3d1fe073e194e8b5b","summary":[{"filename":"jstests/sharding/transient_txn_error_labels_with_write_concern.js","additions":26,"deletions":21,"description":"Server-48053 Fix transient_txn_error_labels_with_write_concern.js for"}]},"message":"Commit Queue Merge: 'Server-48053 Fix transient_txn_error_labels_with_write_concern.js for multiversion testing' into 'mongodb/mongo:main'"}],"activated":true,"alias":"__commit_queue"}
{"_id":{"$oid":"5e4ff3abe3c3317e352062e4"},"triggers":{"child_patches":["5ecedafb562343215a7ff297"]},"desc":"'evergreen-ci/evergreen' pull request #3186 by bsamek: EVG-7425 Don't send ShouldExit to unprovisioned hosts (https://github.com/evergreen-ci/evergreen/pull/3186)","branch":"evergreen","alias":"__commit_queue","githash":"5e823e1f28baeaa22ae00823d83e03082cd148ab","patch_number":2567,"author":"admin","version":"5e4ff3abe3c3317e352062e4","status":"failed","create_time":{"$date":"2020-02-21T15:13:28Z"},"start_time":{"$date":"2020-02-21T15:14:26.122Z"},"finish_time":{"$date":"2020-02-21T15:29:54.869Z"},"project_storage_method":"db","build_variants":["ubuntu1604","lint"],"tasks":["test-model-host","test-model-artifact","test-model-distro","test-cloud","test-validator","test-plugin","test-command","test-db","test-operations","test-units","test-model","test-model-user","test-model-stats","js-test","test-model-commitqueue","test-monitor","test-migrations","test-scheduler","test-util","test-model-notification","test-model-patch","test-rest-route","test-repotracker","test-model-task","test-rest-data","test-model-manifest","test-model-testresult","test-model-grid","test-model-build","test-model-event","test-trigger","test-model-alertrecord","test-service","test-evergreen","test-rest-model","test-auth","generate-lint","test-rest-client","test-thirdparty-docker","test-agent","test-graphql","test-thirdparty"],"variants_tasks":[{"variant":"ubuntu1604","tasks":["test-validator","test-model-artifact","test-model-distro","test-migrations","test-units","test-operations","test-thirdparty","test-trigger","test-model","test-model-user","test-thirdparty-docker","test-cloud","test-rest-client","test-model-alertrecord","test-model-stats","test-plugin","test-service","test-evergreen","test-rest-model","test-agent","test-rest-data","test-util","test-model-notification","test-model-patch","test-auth","test-rest-route","test-model-manifest","test-repotracker","js-test","test-model-task","test-model-commitqueue","test-command","test-graphql","test-model-testresult","test-model-grid","test-monitor","test-model-host","test-model-build","test-model-event","test-db","test-scheduler"],"displaytasks":[]},{"variant":"lint","tasks":["generate-lint"],"displaytasks":[]}],"git_info":{"username":"bsamek","email":"brian@mongodb.com"},"patches":[{"name":"","githash":"5e823e1f28baeaa22ae00823d83e03082cd148ab","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","summary":[{"filename":"service/api_task.go","additions":5,"deletions":4,"description":"ramen is amazing"}]},"message":"","is_mbox":true},{"name":"Spruce","githash":"5e823e1f28baeaa22ae0082383e03082cd148ab","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","summary":[{"filename":"src/pages/Task.tsx","additions":3,"deletions":0,"description":"some other commit"},{"filename":"src/App.tsx","description":"crazy cool commit!!!","additions":0,"deletions":32},{"filename":"src/pages/Patch.tsx","additions":55,"deletions":22,"description":"mega commit"}]},"message":"","is_mbox":true}],"activated":true,"github_patch_data":{"pr_number":3186,"base_owner":"evergreen-ci","base_repo":"evergreen","base_branch":"main","head_owner":"bsamek","head_repo":"evergreen","head_hash":"a7ecb8d2cbaf02ada80e94b70d5d53efe45d9ed6","author":"bsamek","author_uid":624531,"merge_commit_sha":""},"parameters":[{"key":"my_param","value":"my_value"}]}
{"_id":{"$oid":"5e6bb9e23066155a993e0f1a"},"desc":"test meee","branch":"evergreen","githash":"25ab18d7ed2775f27be77d8135ddd841c78cfe28","patch_number":452,"author":"admin","version":"","status":"created","create_time":{"$date":"2020-03-13T16:50:42.981Z"},"start_time":{"$date":{"$numberLong":"-62135596800000"}},"finish_time":{"$date":{"$numberLong":"-62135596800000"}},"project_storage_method":"db","build_variants":["ubuntu1604"],"tasks":["test-graphql"],"variants_tasks":[],"patches":[],"activated":false,"alias":""}
{"_id":{"$oid":"5dd2e89cd1fe07048e43bb9c"},"desc":"'evergreen-ci/spruce' pull request #27 by tgrander: Refactor App + Use Context for global state (https://github.com/evergreen-ci/spruce/pull/27)","branch":"spruce","githash":"3b53f9b61226491cd31113c773d66e351957ed29","patch_number":20,"author":"trey.granderson","version":"5dd2e89cd1fe07048e43bb9c","status":"failed","create_time":{"$date":"2019-11-18T18:53:15Z"},"start_time":{"$date":"2019-11-18T18:54:15.734Z"},"finish_time":{"$date":"2019-11-18T18:57:15.053Z"},"project_storage_method":"db","build_variants":["ubuntu1804"],"tasks":["compile","test","lint","coverage"],"variants_tasks":[{"variant":"ubuntu1804","tasks":["lint","coverage","compile","test"],"displaytasks":[]}],"patches":[{"name":"","githash":"3b53f9b61226491cd31113c773d66e351957ed29","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","summary":[{"filename":"package.json","additions":2,"deletions":0},{"filename":"src/components/Navbar.tsx","additions":70,"deletions":0},{"filename":"src/components/app/App.tsx","additions":78,"deletions":289},{"filename":"src/components/navbar/DevMenu.tsx","additions":62,"deletions":0},{"filename":"src/components/navbar/PluginsMenu.tsx","additions":42,"deletions":0},{"filename":"src/context/ContextProvider.tsx","additions":11,"deletions":0},{"filename":"src/context/apiClient.tsx","additions":70,"deletions":0},{"filename":"src/context/user.tsx","additions":33,"deletions":0},{"filename":"src/utils/isDevelopment.ts","additions":1,"deletions":0},{"filename":"tslint.json","additions":1,"deletions":0}]},"message":""}],"activated":true,"alias":"__github","github_patch_data":{"pr_number":27,"base_owner":"evergreen-ci","base_repo":"spruce","base_branch":"main","head_owner":"evergreen-ci","head_repo":"spruce","head_hash":"2b37dacf86f9d4d1545faaba37c7c5693202e645","author":"tgrander","author_uid":15262143,"merge_commit_sha":""}}
{"_id":{"$oid":"5e94c2dfe3c3312519b59480"},"desc":"SERVER-46893 Allow streamable isMaster to wait on removed/uninitialized nodes","branch":"mongodb-mongo-master","githash":"0170e0a872373b388a48694d24f22da63983e5d0","patch_number":1387.0,"author":"mohamed.khelif","alias":"__commit_queue","version":"5e9748c4e3c331422d0d1d7c","status":"success","create_time":{"$date":"2020-04-15T17:47:49.351Z"},"start_time":{"$date":"2020-04-15T17:49:04.806Z"},"finish_time":{"$date":"2020-04-15T18:04:05.785Z"},"project_storage_method":"db","build_variants":["commit-queue-merge","enterprise-rhel-62-64-bit","commit-queue"],"tasks":["lint_clang_format","lint_eslint","lint_errorcodes","lint_fuzzer_sanity_patch","compile_core_tools","commit_queue_placeholder","merge-patch","lint_yaml","validate_commit_message","lint_cpplint","lint_pylinters","dbtest"],"patches":[{"name":"","githash":"0170e0a872373b388a48694d24f22da63983e5d0","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","summary":[{"filename":"jstests/replsets/awaitable_ismain_errors_on_horizon_change.js","additions":1.0,"deletions":1.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"},{"filename":"jstests/replsets/awaitable_ismain_on_nodes_with_invalid_configs.js","additions":172.0,"deletions":0.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"},{"filename":"src/mongo/db/repl/replication_coordinator.h","additions":2.0,"deletions":2.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"}]},"message":"SERVER-46893 Allow streamable isMaster to wait on removed/uninitialized nodes"}]}
{"_id":{"$oid":"642de18d2a60edf48b34a8c7"},"desc":"EVG-19322: Update local-data","branch":"mongodb-mongo-master","githash":"b06e406ffc5ab54be2dd99672e5c2f7a9082580f","patch_number":4000.0,"author":"sophie.stadler","alias":"__commit_queue","version":"642de18d2a60edf48b34a8c7","status":"created","create_time":{"$date":"2023-04-06T17:47:49.351Z"},"start_time":{"$date":"2023-04-06T17:49:04.806Z"},"finish_time":{"$date":"2023-04-06T18:04:05.785Z"},"project_storage_method":"db","build_variants":["commit-queue-merge","enterprise-rhel-62-64-bit","commit-queue"],"tasks":["lint_clang_format","lint_eslint","lint_errorcodes","lint_fuzzer_sanity_patch","compile_core_tools","commit_queue_placeholder","merge-patch","lint_yaml","validate_commit_message","lint_cpplint","lint_pylinters","dbtest"],"patches":[{"name":"","githash":"0170e0a872373b388a48694d24f22da63983e5d0","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","summary":[{"filename":"jstests/replsets/awaitable_ismain_errors_on_horizon_change.js","additions":1.0,"deletions":1.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"},{"filename":"jstests/replsets/awaitable_ismain_on_nodes_with_invalid_configs.js","additions":172.0,"deletions":0.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"},{"filename":"src/mongo/db/repl/replication_coordinator.h","additions":2.0,"deletions":2.0,"description":"SERVER-46893 Allow streamable isMaster to wait on"}]},"message":"SERVER-46893 Allow streamable isMaster to wait on removed/uninitialized nodes"}]}
{"_id":{"$oid":"52420b363ff1222d23000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["windows-64-2k8-debug"],"create_time":{"$date":"2013-09-24T21:59:18.276Z"},"desc":"","githash":"14837002ef926c56f9ac3683127325833885594f","patches":[{"name":"","githash":"14837002ef926c56f9ac3683127325833885594f","patch_set":{"patch":"diff --git a/SConstruct b/SConstruct\nindex 65987dd..b3cbfe5 100644\n--- a/SConstruct\n+++ b/SConstruct\n@@ -475,6 +475,8 @@ if \"sunos5\" == os.sys.platform:\n     #  http://four.pairlist.net/pipermail/scons-users/2013-June/001486.html\n     env.Tool('gnulink')\n \n+if optBuild:\n+    env.Append( CPPDEFINES=[\"MONGO_OPTIMIZED_BUILD\"] )\n \n if has_option(\"propagate-shell-environment\"):\n     env['ENV'] = dict(os.environ);\ndiff --git a/src/mongo/db/client.cpp b/src/mongo/db/client.cpp\nindex 3c82d7a..737508f 100644\n--- a/src/mongo/db/client.cpp\n+++ b/src/mongo/db/client.cpp\n@@ -81,7 +81,7 @@ namespace mongo {\n \n     TSP_DEFINE(Client, currentClient)\n \n-#if defined(_DEBUG) && !XSAN_ENABLED\n+#if defined(_DEBUG) && !defined(MONGO_OPTIMIZED_BUILD) && !XSAN_ENABLED\n     struct StackChecker;\n     ThreadLocalValue<StackChecker *> checker;\n \n@@ -134,7 +134,7 @@ namespace mongo {\n        call this when your thread starts.\n     */\n     Client& Client::initThread(const char *desc, AbstractMessagingPort *mp) {\n-#if defined(_DEBUG) && !XSAN_ENABLED\n+#if defined(_DEBUG) && !defined(MONGO_OPTIMIZED_BUILD) && !XSAN_ENABLED\n         {\n             if( sizeof(void*) == 8 ) {\n                 StackChecker sc;\n@@ -230,7 +230,7 @@ namespace mongo {\n     }\n \n     bool Client::shutdown() {\n-#if defined(_DEBUG) && !XSAN_ENABLED\n+#if defined(_DEBUG) && !defined(MONGO_OPTIMIZED_BUILD) && !XSAN_ENABLED\n         {\n             if( sizeof(void*) == 8 ) {\n                 StackChecker::check( desc() );\ndiff --git a/src/mongo/dbtests/stacktests.cpp b/src/mongo/dbtests/stacktests.cpp\nindex 5e325d4..0c922af 100644\n--- a/src/mongo/dbtests/stacktests.cpp\n+++ b/src/mongo/dbtests/stacktests.cpp\n@@ -104,7 +104,9 @@ namespace StackTests {\n \n         void setupTests() {\n             if ( inConstructorChainSupported() ) {\n+#if defined(_DEBUG) && !defined(MONGO_OPTIMIZED_BUILD)\n                 DEV add< InCons >(); \n+#endif\n             }\n         }\n         \ndiff --git a/src/mongo/util/stack_introspect.cpp b/src/mongo/util/stack_introspect.cpp\nindex f79d42e..7d6c45e 100644\n--- a/src/mongo/util/stack_introspect.cpp\n+++ b/src/mongo/util/stack_introspect.cpp\n@@ -30,7 +30,7 @@\n \n #include \"mongo/util/stack_introspect.h\"\n \n-#if !defined(_WIN32)\n+#if !defined(_WIN32) && !defined(MONGO_OPTIMIZED_BUILD)\n \n #include <cstdlib>\n #include <cxxabi.h>\n@@ -205,4 +205,4 @@ namespace mongo {\n     bool inConstructorChainSupported() { return false; }\n }\n \n-#endif  // #if !defined(_WIN32)\n+#endif  // #if !defined(_WIN32) && !defined(MONGO_OPTIMIZED_BUILD)\n","summary":[{"filename":"SConstruct","additions":2,"deletions":0},{"filename":"src/mongo/db/client.cpp","additions":3,"deletions":3},{"filename":"src/mongo/dbtests/stacktests.cpp","additions":2,"deletions":0},{"filename":"src/mongo/util/stack_introspect.cpp","additions":2,"deletions":2}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52460a9f3ff1226a3c000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["windows-64","windows-32","solaris-64-bit"],"create_time":{"$date":"2013-09-27T22:45:51.723Z"},"desc":"windows-64,windows-32,solaris-64-bit","githash":"7f3eab9a44d4e3f9fffa8b14fca3b61a444306c4","patches":[{"name":"","githash":"7f3eab9a44d4e3f9fffa8b14fca3b61a444306c4","patch_set":{"patch":"diff --git a/src/mongo/util/net/sock.cpp b/src/mongo/util/net/sock.cpp\nindex a50e8bb..b359bed 100644\n--- a/src/mongo/util/net/sock.cpp\n+++ b/src/mongo/util/net/sock.cpp\n@@ -487,14 +487,20 @@ namespace mongo {\n     public:\n         ConnectBG(int sock, SockAddr remote) : _sock(sock), _remote(remote) { }\n \n-        void run() { _res = ::connect(_sock, _remote.raw(), _remote.addressSize); }\n-        string name() const { return \"ConnectBG\"; }\n+        void run() {\n+            _res = ::connect(_sock, _remote.raw(), _remote.addressSize);\n+            _errnoWithDescription = errnoWithDescription();\n+        }\n+\n+        std::string name() const { return \"ConnectBG\"; }\n+        std::string getErrnoWithDescription() const { return _errnoWithDescription; }\n         int inError() const { return _res; }\n \n     private:\n         int _sock;\n         int _res;\n         SockAddr _remote;\n+        std::string _errnoWithDescription;\n     };\n \n     bool Socket::connect(SockAddr& remote) {\n@@ -514,6 +520,8 @@ namespace mongo {\n         bg.go();\n         if ( bg.wait(5000) ) {\n             if ( bg.inError() ) {\n+                warning() << \"Failed to connect to \" << _remote.getAddr()\n+                          << \", reason: \" << bg.getErrnoWithDescription() << endl;\n                 close();\n                 return false;\n             }\n","summary":[{"filename":"src/mongo/util/net/sock.cpp","additions":10,"deletions":2}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"524994ac3ff1226850000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108-cxx11-debug"],"create_time":{"$date":"2013-09-30T15:11:40.989Z"},"desc":"osx-108-cxx11-debug","githash":"8a2181ba012c8d28faeb5f5dd56cc7e19ae4ec56","patches":[{"name":"","githash":"8a2181ba012c8d28faeb5f5dd56cc7e19ae4ec56","patch_set":{"patch":"diff --git a/src/mongo/util/processinfo.cpp b/src/mongo/util/processinfo.cpp\nindex fd18180..a963ea3 100644\n--- a/src/mongo/util/processinfo.cpp\n+++ b/src/mongo/util/processinfo.cpp\n@@ -38,7 +38,7 @@ namespace mongo {\n             path = p;\n             ofstream out( path.c_str() , ios_base::out );\n             out << ProcessId::getCurrent() << endl;\n-            return out;\n+            return out.good();\n         }\n \n         string path;\n","summary":[{"filename":"src/mongo/util/processinfo.cpp","additions":1,"deletions":1}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"524f0a933ff1221bb8000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-64"],"create_time":{"$date":"2013-10-04T18:36:03.666Z"},"desc":"linux-64","githash":"393194020e81707824b90622cf365333944133ef","patches":[{"name":"","githash":"393194020e81707824b90622cf365333944133ef","patch_set":{"patch":"diff --git a/src/mongo/db/repl/rs_config.cpp b/src/mongo/db/repl/rs_config.cpp\nindex fcef0cc..9b235df 100644\n--- a/src/mongo/db/repl/rs_config.cpp\n+++ b/src/mongo/db/repl/rs_config.cpp\n@@ -74,9 +74,7 @@ namespace mongo {\n         checkRsConfig();\n         log() << \"replSet info saving a newer config version to local.system.replset\" << rsLog;\n         {\n-            Lock::GlobalWrite lk; // TODO: does this really need to be a global lock?\n-            Client::Context cx( rsConfigNs );\n-            cx.db()->flushFiles(true);\n+            Client::WriteContext cx( rsConfigNs );\n \n             //theReplSet->lastOpTimeWritten = ??;\n             //rather than above, do a logOp()? probably\n@@ -85,7 +83,7 @@ namespace mongo {\n             if( !comment.isEmpty() && (!theReplSet || theReplSet->isPrimary()) )\n                 logOpInitiate(comment);\n \n-            cx.db()->flushFiles(true);\n+            cx.ctx().db()->flushFiles(true);\n         }\n         log() << \"replSet saveConfigLocally done\" << rsLog;\n     }\n","summary":[{"filename":"src/mongo/db/repl/rs_config.cpp","additions":2,"deletions":4}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"524f108d3ff1221bb8000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["all"],"create_time":{"$date":"2013-10-04T19:01:33.656Z"},"desc":"all","githash":"37a7f2f07fddce4e413a56753473a7333da35432","patches":[{"name":"","githash":"37a7f2f07fddce4e413a56753473a7333da35432","patch_set":{"patch":"diff --git a/jstests/count10.js b/jstests/count10.js\nindex cb2979d..478c30a 100644\n--- a/jstests/count10.js\n+++ b/jstests/count10.js\n@@ -9,26 +9,27 @@ for ( i=0; i<100; i++ ){\n // make sure data is written\n db.getLastError();\n \n-var thr = new Thread(function () {\n+s = startParallelShell(\n+    'sleep(1000); ' +\n+    'current = db.currentOp({\"ns\": db.count10.getFullName(), \"query.count\": db.count10.getName()}); ' +\n+    'assert(current); ' +\n+    'countOp = current.inprog[0]; ' +\n+    'assert(countOp); ' +\n+    'db.killOp(countOp.opid); '\n+);\n+\n+function getKilledCount() {\n     try {\n         db.count10.find(\"sleep(1000)\").count();\n-    }\n-    catch (e) {\n+    } catch (e) {\n         return e;\n     }\n-});\n-\n-thr.start();\n-sleep(1000);\n-\n-current = db.currentOp({\"ns\": t.getFullName(), \"query.count\": t.getName()});\n-assert(current);\n-countOp = current.inprog[0];\n-assert(countOp);\n-\n-db.killOp(countOp.opid);\n-res = thr.returnData();\n+}\n \n+var res = getKilledCount();\n assert(res);\n assert(res.match(/count failed/) !== null);\n assert(res.match(/\\\"code\\\"/) !== null);\n+\n+s();\n+\n","summary":[{"filename":"jstests/count10.js","additions":16,"deletions":15}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52542ded3ff1221444000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-64-duroff","linux-64-debug-duroff"],"create_time":{"$date":"2013-10-08T16:08:13.059Z"},"desc":"linux-64-duroff,linux-64-debug-duroff","githash":"0b40bfbd29511acf9f075df34c048b1b98cf0094","patches":[{"name":"","githash":"0b40bfbd29511acf9f075df34c048b1b98cf0094","patch_set":{"patch":"diff --git a/jstests/find_and_modify4.js b/jstests/find_and_modify4.js\nindex 5f34318..7af7b0e 100644\n--- a/jstests/find_and_modify4.js\n+++ b/jstests/find_and_modify4.js\n@@ -9,15 +9,15 @@ function getNextVal(counterName){\n                 upsert: true,\n                 'new': true,\n                 });\n-    return ret.val;\n+    return ret;\n }\n \n-assert.eq(getNextVal(\"a\"), 1);\n-assert.eq(getNextVal(\"a\"), 2);\n-assert.eq(getNextVal(\"a\"), 3);\n-assert.eq(getNextVal(\"z\"), 1);\n-assert.eq(getNextVal(\"z\"), 2);\n-assert.eq(getNextVal(\"a\"), 4);\n+assert.eq(getNextVal(\"a\"), {_id:\"a\", val:1});\n+assert.eq(getNextVal(\"a\"), {_id:\"a\", val:2});\n+assert.eq(getNextVal(\"a\"), {_id:\"a\", val:3});\n+assert.eq(getNextVal(\"z\"), {_id:\"z\", val:1});\n+assert.eq(getNextVal(\"z\"), {_id:\"z\", val:2});\n+assert.eq(getNextVal(\"a\"), {_id:\"a\", val:4});\n \n t.drop();\n \ndiff --git a/jstests/upsert1.js b/jstests/upsert1.js\nindex 77cbf57..07b6159 100644\n--- a/jstests/upsert1.js\n+++ b/jstests/upsert1.js\n@@ -1,14 +1,52 @@\n-\n+// tests to make sure that the new _id is returned after the insert\n t = db.upsert1;\n t.drop();\n \n+// make sure the new _id is returned when $mods are used\n t.update( { x : 1 } , { $inc : { y : 1 } } , true );\n l = db.getLastErrorCmd();\n-assert( l.upserted , \"A1\" );\n+assert( l.upserted , \"A1 - \" + tojson(l) );\n assert.eq( l.upserted.str , t.findOne()._id.str , \"A2\" );\n \n+// make sure the new _id is returned on a replacement (no $mod in update)\n t.update( { x : 2 } , { x : 2 , y : 3 } , true );\n l = db.getLastErrorCmd();\n-assert( l.upserted , \"B1\" );\n+assert( l.upserted , \"B1 - \" + tojson(l) );\n assert.eq( l.upserted.str , t.findOne( { x : 2 } )._id.str , \"B2\" );\n assert.eq( 2 , t.find().count() , \"B3\" );\n+\n+// use the _id from the query for the insert\n+t.update({_id:3}, {$set: {a:'123'}}, true)\n+l = db.getLastErrorCmd();\n+assert( l.upserted , \"C1 - \" + tojson(l) );\n+assert.eq( l.upserted , 3 , \"C2 - \" + tojson(l) );\n+\n+// test with an embedded doc for the _id field\n+t.update({_id:{a:1}}, {$set: {a:123}}, true)\n+l = db.getLastErrorCmd();\n+assert( l.upserted , \"D1 - \" + tojson(l) );\n+assert.eq( l.upserted , {a:1} , \"D2 - \" + tojson(l) );\n+\n+// test with a range query\n+t.update({_id: {$gt:100}}, {$set: {a:123}}, true)\n+l = db.getLastErrorCmd();\n+assert( l.upserted , \"E1 - \" + tojson(l) );\n+assert.neq( l.upserted , 100 , \"E2 - \" + tojson(l) );\n+\n+// test with an _id query\n+t.update({_id: 1233}, {$set: {a:123}}, true)\n+l = db.getLastErrorCmd();\n+assert( l.upserted , \"F1 - \" + tojson(l) );\n+assert.eq( l.upserted , 1233 , \"F2 - \" + tojson(l) );\n+\n+// test with an embedded _id query\n+t.update({_id: {a:1, b:2}}, {$set: {a:123}}, true)\n+l = db.getLastErrorCmd();\n+assert( l.upserted , \"G1 - \" + tojson(l) );\n+assert.eq( l.upserted , {a:1, b:2} , \"G2 - \" + tojson(l) );\n+\n+// test with no _id inserted\n+db.createCollection(\"no_id\", {autoIndexId:false})\n+db.no_id.update({foo:1}, {$set:{a:1}}, true)\n+l = db.getLastErrorCmd();\n+assert.eq( l.upserted , undefined , \"H1 - \" + tojson(l) );\ndiff --git a/src/mongo/db/commands/find_and_modify.cpp b/src/mongo/db/commands/find_and_modify.cpp\nindex 0c18cc1..63b4c9f 100644\n--- a/src/mongo/db/commands/find_and_modify.cpp\n+++ b/src/mongo/db/commands/find_and_modify.cpp\n@@ -136,7 +136,9 @@ namespace mongo {\n \n             BSONObj doc;\n             \n+            LOG(0) << \"finding first doc based on query: \" << queryOriginal << \" in \" << ns;\n             bool found = Helpers::findOne( ns.c_str() , queryOriginal , doc );\n+            LOG(0) << \"found doc: \" << doc;\n \n             BSONObj queryModified = queryOriginal;\n             if ( found && doc[\"_id\"].type() && ! isSimpleIdQuery( queryOriginal ) ) {\n@@ -228,14 +230,20 @@ namespace mongo {\n \n                     UpdateResult res = mongo::update(request, &cc().curop()->debug());\n \n+                    LOG(0) << \"update result: \"  << res ;\n                     if ( returnNew ) {\n-                        if ( res.upserted.isSet() ) {\n-                            queryModified = BSON( \"_id\" << res.upserted );\n+                        if ( !res.upserted.isEmpty() ) {\n+                            BSONElement upsertedElem = res.upserted[kUpsertedFieldName];\n+                            LOG(0) << \"using new _id to get new doc: \"\n+                                   << upsertedElem;\n+                            queryModified = upsertedElem.wrap(\"_id\");\n                         }\n                         else if ( queryModified[\"_id\"].type() ) {\n                             // we do this so that if the update changes the fields, it still matches\n                             queryModified = queryModified[\"_id\"].wrap();\n                         }\n+\n+                        LOG(0) << \"using modified query to return the new doc: \" << queryModified;\n                         if ( ! Helpers::findOne( ns.c_str() , queryModified , doc ) ) {\n                             errmsg = str::stream() << \"can't find object after modification  \" \n                                                    << \" ns: \" << ns \n@@ -250,8 +258,9 @@ namespace mongo {\n                     BSONObjBuilder le( result.subobjStart( \"lastErrorObject\" ) );\n                     le.appendBool( \"updatedExisting\" , res.existing );\n                     le.appendNumber( \"n\" , res.numMatched );\n-                    if ( res.upserted.isSet() )\n-                        le.append( \"upserted\" , res.upserted );\n+                    if ( !res.upserted.isEmpty() ) {\n+                        le.append( res.upserted[kUpsertedFieldName] );\n+                    }\n                     le.done();\n                     \n                 }\n@@ -306,11 +315,14 @@ namespace mongo {\n                 }\n \n                 if (cmdObj[\"new\"].trueValue()) {\n-                    BSONElement _id = gle[\"upserted\"];\n-                    if (_id.eoo())\n-                        _id = origQuery[\"_id\"];\n-\n-                    out = db.findOne(ns, QUERY(\"_id\" << _id), fields);\n+                    BSONObjBuilder bob;\n+                    BSONElement _id = gle[kUpsertedFieldName];\n+                    if (!_id.eoo())\n+                        bob.appendAs(_id, \"_id\");\n+                    else\n+                        bob.appendAs(origQuery[\"_id\"], \"_id\");\n+\n+                    out = db.findOne(ns, bob.done(), fields);\n                 }\n \n             }\ndiff --git a/src/mongo/db/commands/write_commands/batch_executor.cpp b/src/mongo/db/commands/write_commands/batch_executor.cpp\nindex 3b10944..626a32a 100644\n--- a/src/mongo/db/commands/write_commands/batch_executor.cpp\n+++ b/src/mongo/db/commands/write_commands/batch_executor.cpp\n@@ -276,7 +276,7 @@ namespace mongo {\n \n         bool resExisting = false;\n         long long resNum = 0;\n-        OID resUpserted = OID();\n+        BSONObj resUpserted;\n         try {\n \n             const NamespaceString requestNs( ns );\n@@ -294,8 +294,8 @@ namespace mongo {\n             resNum = res.numMatched;\n             resUpserted = res.upserted;\n \n-            stats->numUpdated += !resUpserted.isSet() ? resNum : 0;\n-            stats->numUpserted += resUpserted.isSet() ? 1 : 0;\n+            stats->numUpdated += resUpserted.isEmpty() ? resNum : 0;\n+            stats->numUpserted += !resUpserted.isEmpty() ? 1 : 0;\n         }\n         catch ( const UserException& ex ) {\n             opDebug.exceptionInfo = ex.getInfo();\ndiff --git a/src/mongo/db/lasterror.cpp b/src/mongo/db/lasterror.cpp\nindex e239ac8..fd9129e 100644\n--- a/src/mongo/db/lasterror.cpp\n+++ b/src/mongo/db/lasterror.cpp\n@@ -77,9 +77,9 @@ namespace mongo {\n             b.append( \"code\" , code );\n         if ( updatedExisting != NotUpdate )\n             b.appendBool( \"updatedExisting\", updatedExisting == True );\n-        if ( upsertedId.isSet() )\n-            b.append( \"upserted\" , upsertedId );\n-\n+        if ( !upsertedId.isEmpty() ) {\n+            b.append( upsertedId[kUpsertedFieldName] );\n+        }\n         b.appendNumber( \"n\", nObjects );\n \n         return ! msg.empty();\ndiff --git a/src/mongo/db/lasterror.h b/src/mongo/db/lasterror.h\nindex 65c1a72..4a4e7b1 100644\n--- a/src/mongo/db/lasterror.h\n+++ b/src/mongo/db/lasterror.h\n@@ -20,6 +20,7 @@\n #include <boost/thread/tss.hpp>\n #include <string>\n \n+#include \"mongo/db/jsobj.h\"\n #include \"mongo/bson/oid.h\"\n #include \"mongo/util/log.h\"\n \n@@ -27,11 +28,14 @@ namespace mongo {\n     class BSONObjBuilder;\n     class Message;\n \n+    static const char kUpsertedFieldName[] = \"upserted\";\n+\n     struct LastError {\n         int code;\n         std::string msg;\n         enum UpdatedExistingType { NotUpdate, True, False } updatedExisting;\n-        OID upsertedId;\n+        // _id field value from inserted doc, returned as kUpsertedFieldName (above)\n+        BSONObj upsertedId;\n         OID writebackId; // this shouldn't get reset so that old GLE are handled\n         int writebackSince;\n         long long nObjects;\n@@ -48,11 +52,11 @@ namespace mongo {\n             code = _code;\n             msg = _msg;\n         }\n-        void recordUpdate( bool _updateObjects , long long _nObjects , OID _upsertedId ) {\n+        void recordUpdate( bool _updateObjects , long long _nObjects , BSONObj _upsertedId ) {\n             reset( true );\n             nObjects = _nObjects;\n             updatedExisting = _updateObjects ? True : False;\n-            if ( _upsertedId.isSet() )\n+            if ( _upsertedId.valid() && _upsertedId.hasField(kUpsertedFieldName) )\n                 upsertedId = _upsertedId;\n \n         }\n@@ -72,7 +76,7 @@ namespace mongo {\n             nPrev = 1;\n             valid = _valid;\n             disabled = false;\n-            upsertedId.clear();\n+            upsertedId = BSONObj();\n         }\n \n         /**\ndiff --git a/src/mongo/db/ops/update.cpp b/src/mongo/db/ops/update.cpp\nindex 2ada28b..c01eabc 100644\n--- a/src/mongo/db/ops/update.cpp\n+++ b/src/mongo/db/ops/update.cpp\n@@ -117,6 +117,7 @@ namespace mongo {\n \n     UpdateResult update(const UpdateRequest& request, OpDebug* opDebug, UpdateDriver* driver) {\n \n+        LOG(0) << \"processing update : \" << request;\n         const NamespaceString& nsString = request.getNamespaceString();\n \n         validateUpdate( nsString.ns().c_str(), request.getUpdates(), request.getQuery() );\ndiff --git a/src/mongo/db/ops/update_request.h b/src/mongo/db/ops/update_request.h\nindex c2dfb36..0eb5ffe 100644\n--- a/src/mongo/db/ops/update_request.h\n+++ b/src/mongo/db/ops/update_request.h\n@@ -32,9 +32,12 @@\n #include \"mongo/db/curop.h\"\n #include \"mongo/db/namespace_string.h\"\n #include \"mongo/db/query_plan_selection_policy.h\"\n+#include \"mongo/util/mongoutils/str.h\"\n \n namespace mongo {\n \n+    namespace str = mongoutils::str;\n+\n     class UpdateRequest {\n     public:\n         inline UpdateRequest(\n@@ -124,6 +127,17 @@ namespace mongo {\n             return _fromReplication;\n         }\n \n+        const std::string toString() const {\n+            return str::stream()\n+                        << \" query: \" << _query\n+                        << \" updated: \" << _updates\n+                        << \" god: \" << _god\n+                        << \" upsert: \" << _upsert\n+                        << \" multe: \" << _multi\n+                        << \" logToOplog: \" << _updateOpLog\n+                        << \" fromMigration: \" << _fromMigration\n+                        << \" fromReplications: \" << _fromReplication;\n+        }\n     private:\n \n         const NamespaceString& _nsString;\ndiff --git a/src/mongo/db/ops/update_result.h b/src/mongo/db/ops/update_result.h\nindex 7983788..9d42af1 100644\n--- a/src/mongo/db/ops/update_result.h\n+++ b/src/mongo/db/ops/update_result.h\n@@ -32,9 +32,12 @@\n #include \"mongo/db/curop.h\"\n #include \"mongo/db/namespace_string.h\"\n #include \"mongo/db/query_plan_selection_policy.h\"\n+#include \"mongo/util/mongoutils/str.h\"\n \n namespace mongo {\n \n+    namespace str = mongoutils::str;\n+\n     struct UpdateResult {\n \n         UpdateResult( bool existing_,\n@@ -45,10 +48,9 @@ namespace mongo {\n             , modifiers(modifiers_)\n             , numMatched(numMatched_) {\n \n-            upserted.clear();\n             BSONElement id = upsertedObject_[\"_id\"];\n-            if ( ! existing && numMatched == 1 && id.type() == jstOID ) {\n-                upserted = id.OID();\n+            if ( ! existing && numMatched == 1 && !id.eoo() ) {\n+                upserted = id.wrap(kUpsertedFieldName);\n             }\n         }\n \n@@ -63,7 +65,15 @@ namespace mongo {\n         const long long numMatched;\n \n         // if something was upserted, the new _id of the object\n-        OID upserted;\n+        BSONObj upserted;\n+\n+        const std::string toString() const {\n+            return str::stream()\n+                        << \" upserted: \" << upserted\n+                        << \" modifiers: \" << modifiers\n+                        << \" existing: \" << existing\n+                        << \" numMatched: \" << numMatched;\n+        }\n     };\n \n } // namespace mongo\n","summary":[{"filename":"jstests/find_and_modify4.js","additions":7,"deletions":7},{"filename":"jstests/upsert1.js","additions":41,"deletions":3},{"filename":"src/mongo/db/commands/find_and_modify.cpp","additions":21,"deletions":9},{"filename":"src/mongo/db/commands/write_commands/batch_executor.cpp","additions":3,"deletions":3},{"filename":"src/mongo/db/lasterror.cpp","additions":3,"deletions":3},{"filename":"src/mongo/db/lasterror.h","additions":8,"deletions":4},{"filename":"src/mongo/db/ops/update.cpp","additions":1,"deletions":0},{"filename":"src/mongo/db/ops/update_request.h","additions":14,"deletions":0},{"filename":"src/mongo/db/ops/update_result.h","additions":14,"deletions":4}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"525898bb3ff1227ca1000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-32","rhel-57-64-bit"],"create_time":{"$date":"2013-10-12T00:32:59.233Z"},"desc":"SERVER-10992 SERVER-11130 test run","githash":"50682334073f0289d8700080a09299e6e061276c","patches":[{"name":"","githash":"50682334073f0289d8700080a09299e6e061276c","patch_set":{"patch":"diff --git a/src/SConscript.client b/src/SConscript.client\nindex 1690233..06c2645 100644\n--- a/src/SConscript.client\n+++ b/src/SConscript.client\n@@ -219,15 +219,14 @@ if buildShared:\n     # InstallVersionedLibrary support is only stable in SCons > 2.3.0, so if you add support\n     # here, be sure to add an EnsuredSconsVersion here as well.\n     sharedLibEnv = env.Clone()\n-    sharedLibEnv.AppendUnique(\n-        LIBS=mongoClientLibs + mongoClientSysLibDeps,\n-        # TODO: This currently causes the files for the libdep to get dragged into dependents\n-        # of this shared library, incorrectly. We need to patch up libdeps to treat shared\n-        # libraries as dependency terminals.\n-        LIBDEPS=mongoClientLibDeps)\n+    sharedLibEnv.AppendUnique(LIBS=mongoClientSysLibDeps)\n \n-    if linux:\n-        sharedLibEnv.AppendUnique(SHLINKFLAGS=[\"-Wl,--as-needed\", \"-Wl,-zdefs\"])\n+    # Attach the shim only if we are using the external libraries so that we pick up what\n+    # configure found for us.\n+    if use_system_version_of_library(\"boost\"):\n+        sharedLibEnv.AppendUnique(LIBDEPS=mongoClientLibDeps)\n+        if linux:\n+            sharedLibEnv.AppendUnique(SHLINKFLAGS=[\"-Wl,--as-needed\", \"-Wl,-zdefs\"])\n \n     if windows:\n         # Ignore warning LNK4102, which complains about exposing deleting destructors.\n@@ -238,7 +237,7 @@ if buildShared:\n         # SCons automatically looks for this file in the source list and passes the proper\n         # parameter to MS-LINK automatically.\n         clientObjects.append('mongoclient.def')\n-    \n+\n     mongoClientSharedLib = sharedLibEnv.SharedLibrary('mongoclient', clientObjects)\n \n     mongoClientSharedLibInstall = sharedLibEnv.Install(\n@@ -303,7 +302,7 @@ if buildShared:\n     sharedClientEnv.PrependUnique(\n         LIBS=['mongoclient'],\n         LIBPATH=[\"#/sharedclient\"],\n-        LIBDEPS=['$BUILD_DIR/third_party/shim_boost']\n+        LIBDEPS=mongoClientLibDeps,\n     )\n \n     # Deal with the different lookup models between regular UNIX and Darwin. For regular unix,\n","summary":[{"filename":"src/SConscript.client","additions":9,"deletions":10}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"525d5a463ff122435c000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108-cxx11-debug"],"create_time":{"$date":"2013-10-15T15:07:50.691Z"},"desc":"SERVER-11183 test run","githash":"9fcdfeb195c42e91a49e37212a65fea624d71c13","patches":[{"name":"","githash":"9fcdfeb195c42e91a49e37212a65fea624d71c13","patch_set":{"patch":"diff --git a/src/mongo/dbtests/perftests.cpp b/src/mongo/dbtests/perftests.cpp\nindex d1c1d02..66aba15 100644\n--- a/src/mongo/dbtests/perftests.cpp\n+++ b/src/mongo/dbtests/perftests.cpp\n@@ -1363,7 +1363,17 @@ namespace PerfTests {\n                 add< Throw< thr1 > >();\n                 add< Throw< thr2 > >();\n                 add< Throw< thr3 > >();\n+\n+#if !defined(__clang__) || !defined(MONGO_OPTIMIZED_BUILD) || (__clang_major__ > 3) || ((__clang_major__ == 3) && (__clang_minor__ > 2))\n+                // clang-3.2 (and earlier?) miscompiles this test when optimization is on (see\n+                // SERVER-9767 and SERVER-11183 for additional details, including a link to the\n+                // LLVM ticket and LLVM fix).\n+                //\n+                // TODO: We should consider requiring clang > 3.2 in our configure tests once\n+                // XCode 5 is ubiquitious.\n                 add< Throw< thr4 > >();\n+#endif\n+\n                 add< Timer >();\n                 add< Sleep0Ms >();\n #if defined(__USE_XOPEN2K)\n","summary":[{"filename":"src/mongo/dbtests/perftests.cpp","additions":10,"deletions":0}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"525d85c93ff122435c000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108-cxx11-debug"],"create_time":{"$date":"2013-10-15T18:13:29.821Z"},"desc":"SERVER-11183 test run 2","githash":"506fe5ad46b9a0d083af61f4e52a5726f2656949","patches":[{"name":"","githash":"506fe5ad46b9a0d083af61f4e52a5726f2656949","patch_set":{"patch":"diff --git a/src/mongo/dbtests/perftests.cpp b/src/mongo/dbtests/perftests.cpp\nindex d1c1d02..4081844 100644\n--- a/src/mongo/dbtests/perftests.cpp\n+++ b/src/mongo/dbtests/perftests.cpp\n@@ -1363,7 +1363,23 @@ namespace PerfTests {\n                 add< Throw< thr1 > >();\n                 add< Throw< thr2 > >();\n                 add< Throw< thr3 > >();\n+\n+#if !defined(__clang__) || !defined(MONGO_OPTIMIZED_BUILD)\n+                // clang-3.2 (and earlier?) miscompiles this test when optimization is on (see\n+                // SERVER-9767 and SERVER-11183 for additional details, including a link to the\n+                // LLVM ticket and LLVM fix).\n+                //\n+                // Ideally, the test above would also say\n+                // || (__clang_major__ > 3) || ((__clang_major__ == 3) && (__clang_minor__ > 2))\n+                // so that the test would still run on known good vesrions of clang; see\n+                // comments in SERVER-11183 for why that doesn't work.\n+                //\n+                // TODO: Remove this when we no longer need to support clang-3.2. We should\n+                // also consider requiring clang > 3.2 in our configure tests once XCode 5 is\n+                // ubiquitious.\n                 add< Throw< thr4 > >();\n+#endif\n+\n                 add< Timer >();\n                 add< Sleep0Ms >();\n #if defined(__USE_XOPEN2K)\n","summary":[{"filename":"src/mongo/dbtests/perftests.cpp","additions":16,"deletions":0}]}}],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"526157dc3ff1227116000005"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108"],"create_time":{"$date":"2013-10-18T15:46:36.728Z"},"desc":"MCI-832 test run","githash":"428be313f4ac309d792de049fbab966c19f02bb9","patches":[{"name":"","githash":"428be313f4ac309d792de049fbab966c19f02bb9","patch_set":{"patch":"diff --git a/SConstruct b/SConstruct\nindex b33acbd..5f33a58 100644\n--- a/SConstruct\n+++ b/SConstruct\n@@ -1226,9 +1226,11 @@ def doConfigure(myenv):\n     # This needs to happen before we check for libc++, since it affects whether libc++ is available.\n     if darwin and has_option('osx-version-min'):\n         min_version = get_option('osx-version-min')\n-        if not AddToCCFLAGSIfSupported(myenv, '-mmacosx-version-min=%s' % (min_version)):\n+        min_version_flag = '-mmacosx-version-min=%s' % (min_version)\n+        if not AddToCCFLAGSIfSupported(myenv, min_version_flag):\n             print( \"Can't set minimum OS X version with this compiler\" )\n             Exit(1)\n+        myenv.AppendUnique(LINKFLAGS=[min_version_flag])\n \n     if has_option('libc++'):\n         if not using_clang():\n","summary":[{"filename":"SConstruct","additions":3,"deletions":1}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"5267cc2a3ff1227207000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-64"],"create_time":{"$date":"2013-10-23T13:16:26.335Z"},"desc":"linux-64","githash":"3b87e21f115188af7b47e7c50486133450a5d029","patches":[{"name":"","githash":"3b87e21f115188af7b47e7c50486133450a5d029","patch_set":{"patch":"diff --git a/jstests/sharding/hash_shard_unique_compound.js b/jstests/sharding/hash_shard_unique_compound.js\nnew file mode 100644\nindex 0000000..42f6cc9\n--- /dev/null\n+++ b/jstests/sharding/hash_shard_unique_compound.js\n@@ -0,0 +1,50 @@\n+// Basic test of sharding with a hashed shard key and other unique index\n+// Does 2 things and checks for consistent error:\n+//  1.) shard collection on hashed \"a\", ensure unique index {a:1, b:1}\n+//  2.) reverse order\n+\n+var s = new ShardingTest( { name : jsTestName() , shards : 3 , mongos : 1, verbose : 1 } );\n+var dbname = \"test\";\n+var coll = \"foo\";\n+var ns = dbname + \".\" + coll;\n+var db = s.getDB( dbname );\n+var t = db.getCollection( coll );\n+\n+// Enable sharding on DB\n+var res = db.adminCommand( { enablesharding : dbname } );\n+\n+// for simplicity start by turning off balancer\n+var res = s.stopBalancer();\n+\n+// shard a fresh collection using a hashed shard key\n+t.drop();\n+var res = db.adminCommand( { shardcollection : ns , key : { a : \"hashed\" } } );\n+assert.eq( res.ok , 1 , \"shardcollection didn't work -- \" + tojson(res));\n+db.printShardingStatus();\n+\n+// Create unique index\n+t.ensureIndex({a:1, b:1}, {unique:true})\n+var gle = db.getLastErrorObj();\n+assert(!gle.err, \"unique index failed --\" + tojson(gle))\n+\n+print(\"------ indexes -------\")\n+printjson(t.getIndexes());\n+\n+\n+// Second Part\n+print(\"------ dropping sharded collection -------\")\n+t.drop();\n+\n+//Create unique index\n+t.ensureIndex({a:1, b:1}, {unique:true})\n+var gle = db.getLastErrorObj();\n+assert.eq(gle.err, null, \"unique index failed\")\n+\n+// shard a fresh collection using a hashed shard key\n+var res = db.adminCommand( { shardcollection : ns , key : { a : \"hashed\" } } );\n+assert.eq( res.ok , 1 , \"shardcollection didn't worked 2 -- \" + tojson(res));\n+db.printShardingStatus();\n+print(\"------ indexes 2-------\")\n+printjson(t.getIndexes());\n+\n+s.stop()\ndiff --git a/src/mongo/db/keypattern.h b/src/mongo/db/keypattern.h\nindex 7e79f7f..25331e7 100644\n--- a/src/mongo/db/keypattern.h\n+++ b/src/mongo/db/keypattern.h\n@@ -89,7 +89,7 @@ namespace mongo {\n          * the (potentially) compound key described by 'other'\n          */\n         bool isPrefixOf( const KeyPattern& other ) const {\n-            return _pattern.isPrefixOf( other.toBSON() );\n+            return _pattern.isFieldNamePrefixOf( other.toBSON() );\n         }\n \n         /**\ndiff --git a/src/mongo/s/shardkey.cpp b/src/mongo/s/shardkey.cpp\nindex afd91ea..56eb805 100644\n--- a/src/mongo/s/shardkey.cpp\n+++ b/src/mongo/s/shardkey.cpp\n@@ -72,16 +72,12 @@ namespace mongo {\n         return true;\n     }\n \n-    bool ShardKeyPattern::isPrefixOf( const KeyPattern& otherPattern ) const {\n-        return pattern.isPrefixOf( otherPattern );\n-    }\n-\n     bool ShardKeyPattern::isUniqueIndexCompatible( const KeyPattern& uniqueIndexPattern ) const {\n         if ( ! uniqueIndexPattern.toBSON().isEmpty() &&\n              str::equals( uniqueIndexPattern.toBSON().firstElementFieldName(), \"_id\" ) ){\n             return true;\n         }\n-        return pattern.toBSON().isFieldNamePrefixOf( uniqueIndexPattern.toBSON() );\n+        return pattern.isPrefixOf( uniqueIndexPattern );\n     }\n \n     string ShardKeyPattern::toString() const {\ndiff --git a/src/mongo/s/shardkey.h b/src/mongo/s/shardkey.h\nindex b3e1c46..c9de179 100644\n--- a/src/mongo/s/shardkey.h\n+++ b/src/mongo/s/shardkey.h\n@@ -98,12 +98,6 @@ namespace mongo {\n \n         /**\n          * @return\n-         * true if 'this' is a prefix (not necessarily contained) of 'otherPattern'.\n-         */\n-        bool isPrefixOf( const KeyPattern& otherPattern ) const;\n-\n-        /**\n-         * @return\n          * true if this shard key is compatible with a unique index on 'uniqueIndexPattern'.\n          *      Primarily this just checks whether 'this' is a prefix of 'uniqueIndexPattern',\n          *      However it does not need to be an exact syntactic prefix due to \"hashed\"\ndiff --git a/src/mongo/s/strategy_shard.cpp b/src/mongo/s/strategy_shard.cpp\nindex de5899c..5625583 100644\n--- a/src/mongo/s/strategy_shard.cpp\n+++ b/src/mongo/s/strategy_shard.cpp\n@@ -1343,7 +1343,7 @@ namespace mongo {\n                                  \" key: \" + o[\"key\"].embeddedObjectUserCheck().toString() ,\n                                  IndexDetails::isIdIndexPattern( newIndexKey ) ||\n                                  ! o[\"unique\"].trueValue() ||\n-                                 r.getConfig()->getChunkManager( ns )->getShardKey().isPrefixOf( newIndexKey ) );\n+                                 r.getConfig()->getChunkManager( ns )->getShardKey().isUniqueIndexCompatible( newIndexKey ) );\n \n                         ChunkManagerPtr cm = r.getConfig()->getChunkManager( ns );\n                         verify( cm );\n","summary":[{"filename":"jstests/sharding/hash_shard_unique_compound.js","additions":50,"deletions":0},{"filename":"src/mongo/db/keypattern.h","additions":1,"deletions":1},{"filename":"src/mongo/s/shardkey.cpp","additions":1,"deletions":5},{"filename":"src/mongo/s/shardkey.h","additions":0,"deletions":6},{"filename":"src/mongo/s/strategy_shard.cpp","additions":1,"deletions":1}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"526877833ff1225852000005"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-64"],"create_time":{"$date":"2013-10-24T01:27:31.226Z"},"desc":"linux-64","githash":"ec409bbf8271fa5e10284961fe9106c28cbb6023","patches":[{"name":"","githash":"ec409bbf8271fa5e10284961fe9106c28cbb6023","patch_set":{"patch":"diff --git a/src/mongo/db/mongod_options_test.cpp b/src/mongo/db/mongod_options_test.cpp\nindex 2ec72ca..5b33eb0 100644\n--- a/src/mongo/db/mongod_options_test.cpp\n+++ b/src/mongo/db/mongod_options_test.cpp\n@@ -1155,6 +1155,18 @@ namespace {\n                 ASSERT_EQUALS(iterator->_sources, moe::SourceAll);\n             }\n #endif\n+#if defined(__linux__)\n+            else if (iterator->_dottedName == \"shutdown\") {\n+                ASSERT_EQUALS(iterator->_singleName, \"shutdown\");\n+                ASSERT_EQUALS(iterator->_type, moe::Switch);\n+                ASSERT_EQUALS(iterator->_description, \"kill a running server (for init scripts)\");\n+                ASSERT_EQUALS(iterator->_isVisible, true);\n+                ASSERT_TRUE(iterator->_default.isEmpty());\n+                ASSERT_TRUE(iterator->_implicit.isEmpty());\n+                ASSERT_EQUALS(iterator->_isComposing, false);\n+                ASSERT_EQUALS(iterator->_sources, moe::SourceAll);\n+            }\n+#endif\n             else {\n                 ::mongo::StringBuilder sb;\n                 sb << \"Found extra option: \" << iterator->_dottedName <<\n","summary":[{"filename":"src/mongo/db/mongod_options_test.cpp","additions":12,"deletions":0}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"5269746c3ff1223136000003"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","project_storage_method":"db","build_variants":["cxx11-ubuntu1204-64","enterprise-linux-64-amazon-ami","enterprise-rhel-57-64-bit","enterprise-rhel-62-64-bit","enterprise-suse11-64","enterprise-ubuntu1204-64","enterprise-windows-64","linux-32","linux-32-debug","linux-64","linux-64-debug","linux-64-debug-duroff","linux-64-duroff","osx-108","osx-108-cxx11-debug","osx-108-debug","osx-108-dur-off","rhel-57-64-bit","solaris-64-bit","windows-32","windows-64","windows-64-2k8","windows-64-2k8-debug","windows-64-debug"],"create_time":{"$date":"2013-10-24T19:26:36.161Z"},"desc":"","githash":"bc05262b903baafd4a286008ab3323172d8c3275","patches":[{"name":"","githash":"bc05262b903baafd4a286008ab3323172d8c3275","patch_set":{"patch":"diff --git a/jstests/ssl/ssl_fips.js b/jstests/ssl/ssl_fips.js\nindex 76118a7..7b2da1a 100644\n--- a/jstests/ssl/ssl_fips.js\n+++ b/jstests/ssl/ssl_fips.js\n@@ -1,5 +1,4 @@\n // Test mongod start with FIPS mode enabled\n-if (0) { // SERVER-11005\n ports = allocatePorts(1);\n port1 = ports[0];\n var baseName = \"jstests_ssl_ssl_fips\";\n@@ -10,11 +9,21 @@ var md = startMongod(\"--port\", port1, \"--dbpath\",\n                      \"--sslPEMKeyFile\", \"jstests/libs/server.pem\",\n                      \"--sslFIPSMode\");\n \n-var mongo = runMongoProgram(\"mongo\", \"--port\", port1, \"--ssl\",\n-                            \"--sslPEMKeyFile\", \"jstests/libs/client.pem\",\n-                            \"--sslFIPSMode\",\n-                            \"--eval\", \";\");\n+// if mongod didn't start properly\n+if (md != 0) {\n+    print(\"mongod failed to start, checking for FIPS support\");\n+    assert(rawMongoProgramOutput().match(\n+            /this version of mongodb was not compiled with FIPS support/));\n+}\n+else {\n+    // try connecting shell\n+    var mongo = runMongoProgram(\"mongo\", \"--port\", port1, \"--ssl\",\n+                                \"--sslPEMKeyFile\", \"jstests/libs/client.pem\",\n+                                \"--sslFIPSMode\",\n+                                \"--eval\", \";\");\n \n-// 0 is the exit code for success\n-assert(mongo==0);\n+    // 0 is the exit code for success\n+    assert(mongo==0);\n+    // kill mongod\n+    stopMongod(port1, 9);\n }\n","summary":[{"filename":"jstests/ssl/ssl_fips.js","additions":16,"deletions":7}]}}],"tasks":["ssl"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"526b12693ff1226402000007"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108","osx-108-debug","osx-108-dur-off","osx-108-cxx11-debug"],"create_time":{"$date":"2013-10-26T00:52:57.361Z"},"desc":"Empty patch to run a lot of osx tasks","githash":"78a5cc81af2209e10e1357a41e02f1689b60ef1e","patches":[{"name":"","githash":"78a5cc81af2209e10e1357a41e02f1689b60ef1e","patch_set":{"patch":"diff --git a/README b/README\nindex 3dac687..b0f8a3c 100644\n--- a/README\n+++ b/README\n@@ -1,5 +1,6 @@\n MongoDB README\r\n \r\n+\r\n Welcome to MongoDB!\r\n \r\n COMPONENTS\r\n","summary":[{"filename":"README","additions":1,"deletions":0}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52b705e73ff1220a97000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["linux-32-debug","linux-64-debug","linux-64-debug-duroff","osx-108-debug"],"create_time":{"$date":"2013-12-22T15:31:51.621Z"},"desc":"SERVER-12189 test","githash":"a36903ef715cc962a254904e56b0ac1b7efd922f","patches":[{"name":"","githash":"a36903ef715cc962a254904e56b0ac1b7efd922f","patch_set":{"patch":"diff --git a/SConstruct b/SConstruct\nindex 872316a..ba14eeb 100644\n--- a/SConstruct\n+++ b/SConstruct\n@@ -1229,6 +1229,7 @@ def doConfigure(myenv):\n             Exit(1)\n         myenv.AppendUnique(LINKFLAGS=[min_version_flag])\n \n+    usingLibStdCxx = False\n     if has_option('libc++'):\n         if not using_clang():\n             print( 'libc++ is currently only supported for clang')\n@@ -1238,6 +1239,34 @@ def doConfigure(myenv):\n         else:\n             print( 'libc++ requested, but compiler does not support -stdlib=libc++' )\n             Exit(1)\n+    else:\n+        def CheckLibStdCxx(context):\n+            test_body = \"\"\"\n+            #include <vector>\n+            #if !defined(__GLIBCXX__)\n+            #error\n+            #endif\n+            \"\"\"\n+\n+            context.Message('Checking if we are using libstdc++... ')\n+            ret = context.TryCompile(textwrap.dedent(test_body), \".cpp\")\n+            context.Result(ret)\n+            return ret\n+\n+        conf = Configure(myenv, help=False, custom_tests = {\n+            'CheckLibStdCxx' : CheckLibStdCxx,\n+        })\n+        usingLibStdCxx = conf.CheckLibStdCxx()\n+        conf.Finish()\n+\n+    # If we are uinsg libstdc++ and this is a debug build, turn on the debugging features in\n+    # the library.\n+    if usingLibStdCxx and debugBuild:\n+        # We can't do this if we are using any system C++ libraries.\n+        if (not use_system_version_of_library(\"tcmalloc\") and\n+            not use_system_version_of_library(\"boost\") and\n+            not use_system_version_of_library(\"v8\")):\n+            myenv.Append(CPPDEFINES=[\"_GLIBCXX_DEBUG\"]);\n \n     # Check to see if we are trying to use an outdated libstdc++ in C++11 mode. This is\n     # primarly to help people using clang in C++11 mode on OS X but forgetting to use\n@@ -1245,7 +1274,7 @@ def doConfigure(myenv):\n     # is not workable. Instead, we switch on the fact that std::is_nothrow_constructible wasn't\n     # introduced until libstdc++ 4.6.0. Earlier versions of libstdc++ than 4.6 are unlikely to\n     # work well anyway.\n-    if has_option('c++11') and not has_option('libc++'):\n+    if usingLibStdCxx and has_option('c++11'):\n \n         def CheckModernLibStdCxx(context):\n \n","summary":[{"filename":"SConstruct","additions":30,"deletions":1}]}}],"tasks":["aggregation","aggregation_auth","auth","client","client_auth","clone","clone_auth","core","core_auth","disk","disk_auth","durability","durability_auth","failpoints","failpoints_auth","js","js_auth","js_small_oplog","jsCore","jsCore_auth","jsCore_small_oplog","mongo-perf","mongosTest","mongosTest_auth","multiversion","parallel","qa_repo_multiversion_tests","qa_repo_tests","replicasets","replicasets_auth","replication","replication_auth","sharding","sharding_auth","slow_nightly_tests","slow_weekly_tests","ssl","sslSpecial","tool","tool_auth"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52a630633ff1227909000021"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["osx-108"],"create_time":{"$date":"2013-12-09T21:04:35.417Z"},"desc":"testtest","githash":"ea1bcc2beb6638034c1c811042d399f1b92e5cd2","patches":[{"name":"","githash":"ea1bcc2beb6638034c1c811042d399f1b92e5cd2","patch_set":{"patch":"diff --git a/README b/README\nindex 3dac687..f35e2e1 100644\n--- a/README\n+++ b/README\n@@ -1,4 +1,4 @@\n-MongoDB README\n+dsfjhsadjlkfhMongoDB README\n \n Welcome to MongoDB!\n \n","summary":[{"filename":"superloooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooonnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg name","additions":1,"deletions":1}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52617db33ff1224188000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["cxx11-ubuntu1204-64","enterprise-linux-64-amazon-ami","enterprise-rhel-57-64-bit","enterprise-rhel-62-64-bit","enterprise-suse11-64","enterprise-ubuntu1204-64","enterprise-windows-64","linux-32","linux-32-debug","linux-64","linux-64-debug","linux-64-debug-duroff","linux-64-duroff","osx-108","osx-108-cxx11-debug","osx-108-debug","osx-108-dur-off","rhel-57-64-bit","solaris-64-bit","windows-32","windows-64","windows-64-2k8","windows-64-2k8-debug","windows-64-debug"],"create_time":{"$date":"2013-10-18T18:28:03.253Z"},"desc":"work from code freeze","githash":"363c19f1a8b14398128d81ba7a6db7de058298b0","module_patches":[{"name":"enterprise","githash":"f6b84f931571284f5316fcc145788a82089485e0","patch_set":{"patch":"diff --git a/src/audit/audit_indexes_collections_databases.cpp b/src/audit/audit_indexes_collections_databases.cpp\nindex 79b23e5..cf4f0b2 100644\n--- a/src/audit/audit_indexes_collections_databases.cpp\n+++ b/src/audit/audit_indexes_collections_databases.cpp\n@@ -39,7 +39,7 @@ namespace audit {\n     std::ostream& CreateIndexEvent::putTextDescription(std::ostream& os) const {\n         os << \"Created index \" << _indexname\n            << \" on \" << _nsname\n-           << \" as \" << _indexSpec << '.';\n+           << \" as \" << *_indexSpec << '.';\n         return os;\n     }\n \ndiff --git a/src/audit/audit_user_management.cpp b/src/audit/audit_user_management.cpp\nindex 0a21976..fcf62fe 100644\n--- a/src/audit/audit_user_management.cpp\n+++ b/src/audit/audit_user_management.cpp\n@@ -218,15 +218,17 @@ namespace audit {\n             os << \" with customData \" << *_customData << ',';\n         }\n         bool first = true;\n-        for (std::vector<User::RoleData>::const_iterator role = _roles->begin();\n-                    role != _roles->end();\n-                    role++) {\n-            if (first) {\n-                os << \" with the following roles: \" << role->name;\n-                first = false;\n-            }\n-            else {\n-               os << \", \" << role->name;\n+        if (_roles) {\n+            for (std::vector<User::RoleData>::const_iterator role = _roles->begin();\n+                        role != _roles->end();\n+                        role++) {\n+                if (first) {\n+                    os << \" with the following roles: \" << role->name;\n+                    first = false;\n+                }\n+                else {\n+                   os << \", \" << role->name;\n+                }\n             }\n         }\n         os << '.';\n","summary":[{"filename":"src/audit/audit_indexes_collections_databases.cpp","additions":1,"deletions":1},{"filename":"src/audit/audit_user_management.cpp","additions":11,"deletions":9}]}}],"patches":[{"name":"","githash":"363c19f1a8b14398128d81ba7a6db7de058298b0","patch_set":{"patch":"diff --git a/jstests/slowNightly/ttl1.js b/jstests/slowNightly/ttl1.js\nindex f795d8b..50dae56 100644\n--- a/jstests/slowNightly/ttl1.js\n+++ b/jstests/slowNightly/ttl1.js\n@@ -1,21 +1,33 @@\n /**\n  * Part 1: Simple test of TTL.  Create a new collection with 24 docs, with timestamps at one hour\n  *         intervals, from now-minus-23 hours ago until now.  Also add some docs with non-date\n- *         values.  Then create a TTL index that expires all docs older than ~5.5 hours (20000\n- *         seconds).  Wait 70 seconds (TTL monitor runs every 60) and check that 18 docs deleted.\n- * Part 2: Add a second TTL index on an identical field. The second index expires docs older than\n+ *         values.  Then create a TTL index that expires all docs older than a string. Wait 70\n+ *         seconds (TTL monitor runs every 60) and check that no documents were deleted.\n+ * Part 2: Add a second TTL index that expires all docs older than ~5.5 hours (20000\n+ *         seconds).  Wait 70 seconds and check that 18 docs deleted.\n+ * Part 3: Add a third TTL index on an identical field. The second index expires docs older than\n  *         ~2.8 hours (10000 seconds). Wait 70 seconds and check that 3 more docs deleted.\n  */\n \n+assertEntryMatches = function(array, regex) {\n+    var found = false;\n+    for (i=0; i<array.length; i++) {\n+        if (regex.test(array[i])) {\n+             found = true;\n+        }\n+    }\n+    assert(found,\n+           \"The regex: \" + regex + \" did not match any entries in the array: \" + array.join('\\n'));\n+}\n // Part 1\n var t = db.ttl1;\n t.drop();\n \n var now = (new Date()).getTime();\n \n-for ( i=0; i<24; i++ ){\n-    var past = new Date( now - ( 3600 * 1000 * i ) );\n-    t.insert( { x : past , y : past } );\n+for (i=0; i<24; i++) {\n+    var past = new Date(now - (3600 * 1000 * i));\n+    t.insert({x: past, y: past, z: past});\n }\n t.insert( { a : 1 } )     //no x value\n t.insert( { x: null } )   //non-date value\n@@ -27,6 +39,18 @@ db.getLastError();\n \n assert.eq( 30 , t.count() );\n \n+t.ensureIndex( { z : 1 } , { expireAfterSeconds : \"20000\" } );\n+\n+sleep(70 * 1000);\n+\n+assert.eq(t.count(), 30);\n+\n+var loggedWarning = false;\n+var log = db.adminCommand({getLog: \"global\"}).log;\n+var msg = RegExp(\"ttl indexes require the expireAfterSeconds\" +\n+                 \" field to be numeric but received a type of:\");\n+assertEntryMatches(log, msg);\n+// Part 2\n t.ensureIndex( { x : 1 } , { expireAfterSeconds : 20000 } );\n \n assert.soon( \n@@ -41,7 +65,7 @@ assert.eq( 12 , t.count() );\n assert.lte( 18, db.serverStatus().metrics.ttl.deletedDocuments );\n assert.lte( 1, db.serverStatus().metrics.ttl.passes );\n \n-// Part 2\n+// Part 3\n t.ensureIndex( { y : 1 } , { expireAfterSeconds : 10000 } );\n \n assert.soon(\ndiff --git a/src/mongo/db/index_rebuilder.cpp b/src/mongo/db/index_rebuilder.cpp\nindex 6f3edbe..430ec1c 100644\n--- a/src/mongo/db/index_rebuilder.cpp\n+++ b/src/mongo/db/index_rebuilder.cpp\n@@ -54,27 +54,21 @@ namespace mongo {\n         std::vector<std::string> dbNames;\n         getDatabaseNames(dbNames);\n \n-        std::vector<std::string> nsToCheck;\n         try {\n-            for (std::vector<std::string>::const_iterator it = dbNames.begin();\n-                 it < dbNames.end();\n-                 it++) {\n-                const std::string systemNS = *it + \".system.namespaces\";\n-                DBDirectClient cli;\n-                scoped_ptr<DBClientCursor> cursor(cli.query(systemNS, Query()));\n-\n-                // This depends on system.namespaces not changing while we iterate\n-                while (cursor->more()) {\n-                    BSONObj nsDoc = cursor->nextSafe();\n-                    nsToCheck.push_back(nsDoc[\"name\"].valuestrsafe());\n-                }\n+            std::list<std::string> collNames;\n+            for (std::vector<std::string>::const_iterator dbName = dbNames.begin();\n+                 dbName < dbNames.end();\n+                 dbName++) {\n+                NamespaceIndex nsi(storageGlobalParams.dbpath, *dbName);\n+\n+                nsi.getNamespaces(collNames, /* onlyCollections */ true);\n             }\n             {\n                 boost::unique_lock<boost::mutex> lk(ReplSet::rss.mtx);\n                 ReplSet::rss.indexRebuildDone = true;\n                 ReplSet::rss.cond.notify_all();\n             }\n-            checkNS(nsToCheck);\n+            checkNS(collNames);\n         }\n         catch (const DBException&) {\n             warning() << \"index rebuilding did not complete\" << endl;\n@@ -87,9 +81,9 @@ namespace mongo {\n         LOG(1) << \"checking complete\" << endl;\n     }\n \n-    void IndexRebuilder::checkNS(const std::vector<std::string>& nsToCheck) {\n+    void IndexRebuilder::checkNS(const std::list<std::string>& nsToCheck) {\n         bool firstTime = true;\n-        for (std::vector<std::string>::const_iterator it = nsToCheck.begin();\n+        for (std::list<std::string>::const_iterator it = nsToCheck.begin();\n                 it != nsToCheck.end();\n                 ++it) {\n             // This write lock is held throughout the index building process\ndiff --git a/src/mongo/db/index_rebuilder.h b/src/mongo/db/index_rebuilder.h\nindex 08f10f7..110736f 100644\n--- a/src/mongo/db/index_rebuilder.h\n+++ b/src/mongo/db/index_rebuilder.h\n@@ -44,10 +44,10 @@ namespace mongo {\n \n     private:\n         /**\n-         * Check each collection in the passed in vector to see if it has any in-progress index\n+         * Check each collection in the passed in list to see if it has any in-progress index\n          * builds that need to be retried.  If so, calls retryIndexBuild.\n          */\n-        void checkNS(const std::vector<std::string>& nsToCheck);\n+        void checkNS(const std::list<std::string>& nsToCheck);\n \n         /**\n          * Actually retry an index build on a given namespace.\ndiff --git a/src/mongo/db/repl/rs.cpp b/src/mongo/db/repl/rs.cpp\nindex b67cf17..a096f78 100644\n--- a/src/mongo/db/repl/rs.cpp\n+++ b/src/mongo/db/repl/rs.cpp\n@@ -644,21 +644,11 @@ namespace {\n         // this is a shortcut for simple changes\n         if( additive ) {\n             log() << \"replSet info : additive change to configuration\" << rsLog;\n-            for( list<ReplSetConfig::MemberCfg*>::const_iterator i = newOnes.begin(); i != newOnes.end(); i++ ) {\n-                ReplSetConfig::MemberCfg *m = *i;\n-                Member *mi = new Member(m->h, m->_id, m, false);\n-\n-                /** we will indicate that new members are up() initially so that we don't relinquish our\n-                    primary state because we can't (transiently) see a majority.  they should be up as we\n-                    check that new members are up before getting here on reconfig anyway.\n-                    */\n-                mi->get_hbinfo().health = 0.1;\n-\n-                _members.push(mi);\n-                startHealthTaskFor(mi);\n-            }\n-\n             if (updateConfigs) {\n+                // we have new configs for existing members, so we need to repopulate _members\n+                // with the most recent configs\n+                _members.orphanAll();\n+\n                 // for logging\n                 string members = \"\";\n \n@@ -684,6 +674,22 @@ namespace {\n                 }\n             }\n \n+            // add any new members\n+            for (list<ReplSetConfig::MemberCfg*>::const_iterator i = newOnes.begin();\n+                    i != newOnes.end();\n+                    i++) {\n+                ReplSetConfig::MemberCfg *m = *i;\n+                Member *mi = new Member(m->h, m->_id, m, false);\n+\n+                // we will indicate that new members are up() initially so that we don't relinquish\n+                // our primary state because we can't (transiently) see a majority. they should be\n+                // up as we check that new members are up before getting here on reconfig anyway.\n+                mi->get_hbinfo().health = 0.1;\n+\n+                _members.push(mi);\n+                startHealthTaskFor(mi);\n+            }\n+\n             // if we aren't creating new members, we may have to update the\n             // groups for the current ones\n             _cfg->updateMembers(_members);\ndiff --git a/src/mongo/db/repl/rs_sync.cpp b/src/mongo/db/repl/rs_sync.cpp\nindex bfa5442..f244003 100644\n--- a/src/mongo/db/repl/rs_sync.cpp\n+++ b/src/mongo/db/repl/rs_sync.cpp\n@@ -258,7 +258,7 @@ namespace replset {\n         \n         std::vector< std::vector<BSONObj> > writerVectors(theReplSet->replWriterThreadCount);\n         fillWriterVectors(ops, &writerVectors);\n-        LOG(1) << \"replication batch size is \" << ops.size() << endl;\n+        LOG(2) << \"replication batch size is \" << ops.size() << endl;\n         // We must grab this because we're going to grab write locks later.\n         // We hold this mutex the entire time we're writing; it doesn't matter\n         // because all readers are blocked anyway.\ndiff --git a/src/mongo/db/server_options.cpp b/src/mongo/db/server_options.cpp\nindex bbd60e8..46deb38 100644\n--- a/src/mongo/db/server_options.cpp\n+++ b/src/mongo/db/server_options.cpp\n@@ -34,6 +34,7 @@\n #define SYSLOG_NAMES\n #include <syslog.h>\n #endif\n+#include <boost/filesystem.hpp>\n \n #include \"mongo/base/status.h\"\n #include \"mongo/bson/util/builder.h\"\n@@ -586,7 +587,8 @@ namespace {\n         }\n \n         if (params.count(\"keyFile\")) {\n-            serverGlobalParams.keyFile = params[\"keyFile\"].as<string>();\n+            serverGlobalParams.keyFile = boost::filesystem::absolute(\n+                                            params[\"keyFile\"].as<string>()).generic_string();\n         }\n \n         if ( params.count(\"pidfilepath\")) {\ndiff --git a/src/mongo/db/structure/collection_info_cache.cpp b/src/mongo/db/structure/collection_info_cache.cpp\nindex 8643446..7115384 100644\n--- a/src/mongo/db/structure/collection_info_cache.cpp\n+++ b/src/mongo/db/structure/collection_info_cache.cpp\n@@ -42,8 +42,10 @@\n namespace mongo {\n \n     CollectionInfoCache::CollectionInfoCache( Collection* collection )\n-        : _collection( collection ), _qcCacheMutex( \"_qcCacheMutex\" ) {\n-    }\n+        : _collection( collection ),\n+          _keysComputed( false ),\n+          _qcCacheMutex( \"_qcCacheMutex\" ),\n+          _qcWriteCount( 0 ) {}\n \n     void CollectionInfoCache::reset() {\n         Lock::assertWriteLocked( _collection->ns().ns() );\ndiff --git a/src/mongo/db/ttl.cpp b/src/mongo/db/ttl.cpp\nindex 88d83fe..15d5dbe 100644\n--- a/src/mongo/db/ttl.cpp\n+++ b/src/mongo/db/ttl.cpp\n@@ -92,6 +92,12 @@ namespace mongo {\n                     error() << \"key for ttl index can only have 1 field\" << endl;\n                     continue;\n                 }\n+                if (!idx[secondsExpireField].isNumber()) {\n+                    log() << \"ttl indexes require the \" << secondsExpireField << \" field to be \"\n+                          << \"numeric but received a type of: \"\n+                          << typeName(idx[secondsExpireField].type());\n+                    continue;\n+                }\n \n                 BSONObj query;\n                 {\n","summary":[{"filename":"jstests/slowNightly/ttl1.js","additions":31,"deletions":7},{"filename":"src/mongo/db/index_rebuilder.cpp","additions":10,"deletions":16},{"filename":"src/mongo/db/index_rebuilder.h","additions":2,"deletions":2},{"filename":"src/mongo/db/repl/rs.cpp","additions":20,"deletions":14},{"filename":"src/mongo/db/repl/rs_sync.cpp","additions":1,"deletions":1},{"filename":"src/mongo/db/server_options.cpp","additions":3,"deletions":1},{"filename":"src/mongo/db/structure/collection_info_cache.cpp","additions":4,"deletions":2},{"filename":"src/mongo/db/ttl.cpp","additions":6,"deletions":0}]}}],"tasks":["slow_nightly_tests","core","core_auth","failpoints","failpoints_auth","disk","disk_auth","tool","tool_auth","auth","client","client_auth","js","js_auth","js_small_oplog","replicasets","replicasets_auth","durability","durability_auth","mongosTest","mongosTest_auth","sharding","sharding_auth","replication","replication_auth","clone","clone_auth","parallel","aggregation","aggregation_auth","slow_weekly_tests","ssl","mongo-perf"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52653f463ff1222a2f000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["cxx11-ubuntu1204-64","enterprise-linux-64-amazon-ami","enterprise-rhel-57-64-bit","enterprise-rhel-62-64-bit","enterprise-suse11-64","enterprise-ubuntu1204-64","enterprise-windows-64","linux-32","linux-32-debug","linux-64","linux-64-debug","linux-64-debug-duroff","linux-64-duroff","osx-108","osx-108-cxx11-debug","osx-108-debug","osx-108-dur-off","rhel-57-64-bit","solaris-64-bit","windows-32","windows-64","windows-64-2k8","windows-64-2k8-debug","windows-64-debug"],"create_time":{"$date":"2013-10-21T14:50:46.251Z"},"desc":"","githash":"04c63390531b22857bd8110761b5cf631278f9b6","module_patches":[{"name":"enterprise","githash":"f6b84f931571284f5316fcc145788a82089485e0","patch_set":{"patch":"diff --git a/src/audit/audit_indexes_collections_databases.cpp b/src/audit/audit_indexes_collections_databases.cpp\nindex 79b23e5..cf4f0b2 100644\n--- a/src/audit/audit_indexes_collections_databases.cpp\n+++ b/src/audit/audit_indexes_collections_databases.cpp\n@@ -39,7 +39,7 @@ namespace audit {\n     std::ostream& CreateIndexEvent::putTextDescription(std::ostream& os) const {\n         os << \"Created index \" << _indexname\n            << \" on \" << _nsname\n-           << \" as \" << _indexSpec << '.';\n+           << \" as \" << *_indexSpec << '.';\n         return os;\n     }\n \ndiff --git a/src/audit/audit_user_management.cpp b/src/audit/audit_user_management.cpp\nindex 0a21976..fcf62fe 100644\n--- a/src/audit/audit_user_management.cpp\n+++ b/src/audit/audit_user_management.cpp\n@@ -218,15 +218,17 @@ namespace audit {\n             os << \" with customData \" << *_customData << ',';\n         }\n         bool first = true;\n-        for (std::vector<User::RoleData>::const_iterator role = _roles->begin();\n-                    role != _roles->end();\n-                    role++) {\n-            if (first) {\n-                os << \" with the following roles: \" << role->name;\n-                first = false;\n-            }\n-            else {\n-               os << \", \" << role->name;\n+        if (_roles) {\n+            for (std::vector<User::RoleData>::const_iterator role = _roles->begin();\n+                        role != _roles->end();\n+                        role++) {\n+                if (first) {\n+                    os << \" with the following roles: \" << role->name;\n+                    first = false;\n+                }\n+                else {\n+                   os << \", \" << role->name;\n+                }\n             }\n         }\n         os << '.';\n","summary":[{"filename":"src/audit/audit_indexes_collections_databases.cpp","additions":1,"deletions":1},{"filename":"src/audit/audit_user_management.cpp","additions":11,"deletions":9}]}}],"patches":[{"name":"","githash":"04c63390531b22857bd8110761b5cf631278f9b6","patch_set":{"patch":"diff --git a/jstests/slowNightly/ttl1.js b/jstests/slowNightly/ttl1.js\nindex f795d8b..50dae56 100644\n--- a/jstests/slowNightly/ttl1.js\n+++ b/jstests/slowNightly/ttl1.js\n@@ -1,21 +1,33 @@\n /**\n  * Part 1: Simple test of TTL.  Create a new collection with 24 docs, with timestamps at one hour\n  *         intervals, from now-minus-23 hours ago until now.  Also add some docs with non-date\n- *         values.  Then create a TTL index that expires all docs older than ~5.5 hours (20000\n- *         seconds).  Wait 70 seconds (TTL monitor runs every 60) and check that 18 docs deleted.\n- * Part 2: Add a second TTL index on an identical field. The second index expires docs older than\n+ *         values.  Then create a TTL index that expires all docs older than a string. Wait 70\n+ *         seconds (TTL monitor runs every 60) and check that no documents were deleted.\n+ * Part 2: Add a second TTL index that expires all docs older than ~5.5 hours (20000\n+ *         seconds).  Wait 70 seconds and check that 18 docs deleted.\n+ * Part 3: Add a third TTL index on an identical field. The second index expires docs older than\n  *         ~2.8 hours (10000 seconds). Wait 70 seconds and check that 3 more docs deleted.\n  */\n \n+assertEntryMatches = function(array, regex) {\n+    var found = false;\n+    for (i=0; i<array.length; i++) {\n+        if (regex.test(array[i])) {\n+             found = true;\n+        }\n+    }\n+    assert(found,\n+           \"The regex: \" + regex + \" did not match any entries in the array: \" + array.join('\\n'));\n+}\n // Part 1\n var t = db.ttl1;\n t.drop();\n \n var now = (new Date()).getTime();\n \n-for ( i=0; i<24; i++ ){\n-    var past = new Date( now - ( 3600 * 1000 * i ) );\n-    t.insert( { x : past , y : past } );\n+for (i=0; i<24; i++) {\n+    var past = new Date(now - (3600 * 1000 * i));\n+    t.insert({x: past, y: past, z: past});\n }\n t.insert( { a : 1 } )     //no x value\n t.insert( { x: null } )   //non-date value\n@@ -27,6 +39,18 @@ db.getLastError();\n \n assert.eq( 30 , t.count() );\n \n+t.ensureIndex( { z : 1 } , { expireAfterSeconds : \"20000\" } );\n+\n+sleep(70 * 1000);\n+\n+assert.eq(t.count(), 30);\n+\n+var loggedWarning = false;\n+var log = db.adminCommand({getLog: \"global\"}).log;\n+var msg = RegExp(\"ttl indexes require the expireAfterSeconds\" +\n+                 \" field to be numeric but received a type of:\");\n+assertEntryMatches(log, msg);\n+// Part 2\n t.ensureIndex( { x : 1 } , { expireAfterSeconds : 20000 } );\n \n assert.soon( \n@@ -41,7 +65,7 @@ assert.eq( 12 , t.count() );\n assert.lte( 18, db.serverStatus().metrics.ttl.deletedDocuments );\n assert.lte( 1, db.serverStatus().metrics.ttl.passes );\n \n-// Part 2\n+// Part 3\n t.ensureIndex( { y : 1 } , { expireAfterSeconds : 10000 } );\n \n assert.soon(\ndiff --git a/src/mongo/db/index_rebuilder.cpp b/src/mongo/db/index_rebuilder.cpp\nindex 6f3edbe..430ec1c 100644\n--- a/src/mongo/db/index_rebuilder.cpp\n+++ b/src/mongo/db/index_rebuilder.cpp\n@@ -54,27 +54,21 @@ namespace mongo {\n         std::vector<std::string> dbNames;\n         getDatabaseNames(dbNames);\n \n-        std::vector<std::string> nsToCheck;\n         try {\n-            for (std::vector<std::string>::const_iterator it = dbNames.begin();\n-                 it < dbNames.end();\n-                 it++) {\n-                const std::string systemNS = *it + \".system.namespaces\";\n-                DBDirectClient cli;\n-                scoped_ptr<DBClientCursor> cursor(cli.query(systemNS, Query()));\n-\n-                // This depends on system.namespaces not changing while we iterate\n-                while (cursor->more()) {\n-                    BSONObj nsDoc = cursor->nextSafe();\n-                    nsToCheck.push_back(nsDoc[\"name\"].valuestrsafe());\n-                }\n+            std::list<std::string> collNames;\n+            for (std::vector<std::string>::const_iterator dbName = dbNames.begin();\n+                 dbName < dbNames.end();\n+                 dbName++) {\n+                NamespaceIndex nsi(storageGlobalParams.dbpath, *dbName);\n+\n+                nsi.getNamespaces(collNames, /* onlyCollections */ true);\n             }\n             {\n                 boost::unique_lock<boost::mutex> lk(ReplSet::rss.mtx);\n                 ReplSet::rss.indexRebuildDone = true;\n                 ReplSet::rss.cond.notify_all();\n             }\n-            checkNS(nsToCheck);\n+            checkNS(collNames);\n         }\n         catch (const DBException&) {\n             warning() << \"index rebuilding did not complete\" << endl;\n@@ -87,9 +81,9 @@ namespace mongo {\n         LOG(1) << \"checking complete\" << endl;\n     }\n \n-    void IndexRebuilder::checkNS(const std::vector<std::string>& nsToCheck) {\n+    void IndexRebuilder::checkNS(const std::list<std::string>& nsToCheck) {\n         bool firstTime = true;\n-        for (std::vector<std::string>::const_iterator it = nsToCheck.begin();\n+        for (std::list<std::string>::const_iterator it = nsToCheck.begin();\n                 it != nsToCheck.end();\n                 ++it) {\n             // This write lock is held throughout the index building process\ndiff --git a/src/mongo/db/index_rebuilder.h b/src/mongo/db/index_rebuilder.h\nindex 08f10f7..110736f 100644\n--- a/src/mongo/db/index_rebuilder.h\n+++ b/src/mongo/db/index_rebuilder.h\n@@ -44,10 +44,10 @@ namespace mongo {\n \n     private:\n         /**\n-         * Check each collection in the passed in vector to see if it has any in-progress index\n+         * Check each collection in the passed in list to see if it has any in-progress index\n          * builds that need to be retried.  If so, calls retryIndexBuild.\n          */\n-        void checkNS(const std::vector<std::string>& nsToCheck);\n+        void checkNS(const std::list<std::string>& nsToCheck);\n \n         /**\n          * Actually retry an index build on a given namespace.\ndiff --git a/src/mongo/db/repl/rs.cpp b/src/mongo/db/repl/rs.cpp\nindex b67cf17..a096f78 100644\n--- a/src/mongo/db/repl/rs.cpp\n+++ b/src/mongo/db/repl/rs.cpp\n@@ -644,21 +644,11 @@ namespace {\n         // this is a shortcut for simple changes\n         if( additive ) {\n             log() << \"replSet info : additive change to configuration\" << rsLog;\n-            for( list<ReplSetConfig::MemberCfg*>::const_iterator i = newOnes.begin(); i != newOnes.end(); i++ ) {\n-                ReplSetConfig::MemberCfg *m = *i;\n-                Member *mi = new Member(m->h, m->_id, m, false);\n-\n-                /** we will indicate that new members are up() initially so that we don't relinquish our\n-                    primary state because we can't (transiently) see a majority.  they should be up as we\n-                    check that new members are up before getting here on reconfig anyway.\n-                    */\n-                mi->get_hbinfo().health = 0.1;\n-\n-                _members.push(mi);\n-                startHealthTaskFor(mi);\n-            }\n-\n             if (updateConfigs) {\n+                // we have new configs for existing members, so we need to repopulate _members\n+                // with the most recent configs\n+                _members.orphanAll();\n+\n                 // for logging\n                 string members = \"\";\n \n@@ -684,6 +674,22 @@ namespace {\n                 }\n             }\n \n+            // add any new members\n+            for (list<ReplSetConfig::MemberCfg*>::const_iterator i = newOnes.begin();\n+                    i != newOnes.end();\n+                    i++) {\n+                ReplSetConfig::MemberCfg *m = *i;\n+                Member *mi = new Member(m->h, m->_id, m, false);\n+\n+                // we will indicate that new members are up() initially so that we don't relinquish\n+                // our primary state because we can't (transiently) see a majority. they should be\n+                // up as we check that new members are up before getting here on reconfig anyway.\n+                mi->get_hbinfo().health = 0.1;\n+\n+                _members.push(mi);\n+                startHealthTaskFor(mi);\n+            }\n+\n             // if we aren't creating new members, we may have to update the\n             // groups for the current ones\n             _cfg->updateMembers(_members);\ndiff --git a/src/mongo/db/repl/rs_sync.cpp b/src/mongo/db/repl/rs_sync.cpp\nindex bfa5442..f244003 100644\n--- a/src/mongo/db/repl/rs_sync.cpp\n+++ b/src/mongo/db/repl/rs_sync.cpp\n@@ -258,7 +258,7 @@ namespace replset {\n         \n         std::vector< std::vector<BSONObj> > writerVectors(theReplSet->replWriterThreadCount);\n         fillWriterVectors(ops, &writerVectors);\n-        LOG(1) << \"replication batch size is \" << ops.size() << endl;\n+        LOG(2) << \"replication batch size is \" << ops.size() << endl;\n         // We must grab this because we're going to grab write locks later.\n         // We hold this mutex the entire time we're writing; it doesn't matter\n         // because all readers are blocked anyway.\ndiff --git a/src/mongo/db/server_options.cpp b/src/mongo/db/server_options.cpp\nindex bbd60e8..46deb38 100644\n--- a/src/mongo/db/server_options.cpp\n+++ b/src/mongo/db/server_options.cpp\n@@ -34,6 +34,7 @@\n #define SYSLOG_NAMES\n #include <syslog.h>\n #endif\n+#include <boost/filesystem.hpp>\n \n #include \"mongo/base/status.h\"\n #include \"mongo/bson/util/builder.h\"\n@@ -586,7 +587,8 @@ namespace {\n         }\n \n         if (params.count(\"keyFile\")) {\n-            serverGlobalParams.keyFile = params[\"keyFile\"].as<string>();\n+            serverGlobalParams.keyFile = boost::filesystem::absolute(\n+                                            params[\"keyFile\"].as<string>()).generic_string();\n         }\n \n         if ( params.count(\"pidfilepath\")) {\ndiff --git a/src/mongo/db/structure/collection_info_cache.cpp b/src/mongo/db/structure/collection_info_cache.cpp\nindex 8643446..7115384 100644\n--- a/src/mongo/db/structure/collection_info_cache.cpp\n+++ b/src/mongo/db/structure/collection_info_cache.cpp\n@@ -42,8 +42,10 @@\n namespace mongo {\n \n     CollectionInfoCache::CollectionInfoCache( Collection* collection )\n-        : _collection( collection ), _qcCacheMutex( \"_qcCacheMutex\" ) {\n-    }\n+        : _collection( collection ),\n+          _keysComputed( false ),\n+          _qcCacheMutex( \"_qcCacheMutex\" ),\n+          _qcWriteCount( 0 ) {}\n \n     void CollectionInfoCache::reset() {\n         Lock::assertWriteLocked( _collection->ns().ns() );\ndiff --git a/src/mongo/db/ttl.cpp b/src/mongo/db/ttl.cpp\nindex 88d83fe..15d5dbe 100644\n--- a/src/mongo/db/ttl.cpp\n+++ b/src/mongo/db/ttl.cpp\n@@ -92,6 +92,12 @@ namespace mongo {\n                     error() << \"key for ttl index can only have 1 field\" << endl;\n                     continue;\n                 }\n+                if (!idx[secondsExpireField].isNumber()) {\n+                    log() << \"ttl indexes require the \" << secondsExpireField << \" field to be \"\n+                          << \"numeric but received a type of: \"\n+                          << typeName(idx[secondsExpireField].type());\n+                    continue;\n+                }\n \n                 BSONObj query;\n                 {\n","summary":[{"filename":"jstests/slowNightly/ttl1.js","additions":31,"deletions":7},{"filename":"src/mongo/db/index_rebuilder.cpp","additions":10,"deletions":16},{"filename":"src/mongo/db/index_rebuilder.h","additions":2,"deletions":2},{"filename":"src/mongo/db/repl/rs.cpp","additions":20,"deletions":14},{"filename":"src/mongo/db/repl/rs_sync.cpp","additions":1,"deletions":1},{"filename":"src/mongo/db/server_options.cpp","additions":3,"deletions":1},{"filename":"src/mongo/db/structure/collection_info_cache.cpp","additions":4,"deletions":2},{"filename":"src/mongo/db/ttl.cpp","additions":6,"deletions":0}]}}],"tasks":["slow_nightly_tests","core","core_auth","failpoints","failpoints_auth","disk","disk_auth","tool","tool_auth","auth","client","client_auth","js","js_auth","js_small_oplog","replicasets","replicasets_auth","durability","durability_auth","mongosTest","mongosTest_auth","sharding","sharding_auth","replication","replication_auth","clone","clone_auth","parallel","aggregation","aggregation_auth","slow_weekly_tests","ssl","mongo-perf"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52654a6e3ff1222a2f000002"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["all"],"create_time":{"$date":"2013-10-21T15:38:22.746Z"},"desc":"all","githash":"428be313f4ac309d792de049fbab966c19f02bb9","patches":[{"name":"","githash":"428be313f4ac309d792de049fbab966c19f02bb9","patch_set":{"patch":"diff --git a/jstests/tool/csvexport_dates.js b/jstests/tool/csvexport_dates.js\ndeleted file mode 100644\nindex 6787d1c..0000000\n--- a/jstests/tool/csvexport_dates.js\n+++ /dev/null\n@@ -1,77 +0,0 @@\n-// Test that we can handle various edge cases of a date type in a csv export\n-\n-t = new ToolTest(\"csvexport_date_before_epoch\")\n-\n-c = t.startDB(\"foo\");\n-\n-function test(date) {\n-\n-    if (date.valueOf() < 0 && _isWindows()) {\n-        // some versions of windows (but not all) fail with dates before 1970\n-        print(\"skipping test of \" + date.tojson() +\n-              \" because system might not support dates before 1970\");\n-        return;\n-    }\n-\n-    print(\"testing \" + date);\n-\n-    c.drop();\n-\n-    assert.eq(0, c.count(), \"initial collection not empty\");\n-\n-    c.insert({ _id : 1, date : date });\n-\n-    assert.eq(1, c.count(), \"failed to insert document into collection\");\n-\n-    t.runTool(\"export\", \"--out\", t.extFile, \"-d\", t.baseName, \"-c\", \"foo\", \"--csv\", \"-f\",\n-              \"_id,date\")\n-\n-    c.drop()\n-\n-    assert.eq(0, c.count(), \"failed to drop collection\")\n-\n-    t.runTool(\"import\", \"--file\", t.extFile, \"-d\", t.baseName, \"-c\", \"foo\", \"--type\", \"csv\",\n-              \"--headerline\");\n-\n-    assert.soon(1 + \" == c.count()\", \"after import\");\n-\n-    // Note: Exporting and Importing to/from CSV is not designed to be round-trippable\n-    var expected = { \"date\" : date.toISOString() };\n-\n-    var actual = c.findOne();\n-\n-    delete actual._id\n-    assert.eq(expected, actual, \"imported doc did not match expected\");\n-}\n-\n-// Basic test\n-test(ISODate('1960-01-02 03:04:05.006Z'));\n-\n-// Testing special rounding rules for seconds\n-test(ISODate('1960-01-02 03:04:04.999Z')); // second = 4\n-test(ISODate('1960-01-02 03:04:05.000Z')); // second = 5\n-test(ISODate('1960-01-02 03:04:05.001Z')); // second = 5\n-test(ISODate('1960-01-02 03:04:05.999Z')); // second = 5\n-\n-// Test date before 1900 (negative tm_year values from gmtime)\n-// Skip this test on systems where a time_t is not big enough to store the value\n-buildInfo = t.db.adminCommand(\"buildInfo\");\n-if (buildInfo.bits >= 64) {\n-    test(ISODate('1860-01-02 03:04:05.006Z'));\n-}\n-\n-// Test with time_t == -1 and 0\n-test(new Date(-1000));\n-test(new Date(0));\n-\n-// Testing dates between 1970 and 2000\n-test(ISODate('1970-01-01 00:00:00.000Z'));\n-test(ISODate('1970-01-01 00:00:00.999Z'));\n-test(ISODate('1980-05-20 12:53:64.834Z'));\n-test(ISODate('1999-12-31 00:00:00.000Z'));\n-test(ISODate('1999-12-31 23:59:59.999Z'));\n-\n-// Test date > 2000 for completeness (using now)\n-test(new Date());\n-\n-t.stop();\ndiff --git a/src/mongo/db/pipeline/expression.cpp b/src/mongo/db/pipeline/expression.cpp\nindex cf9eaca..8fb62a3 100644\n--- a/src/mongo/db/pipeline/expression.cpp\n+++ b/src/mongo/db/pipeline/expression.cpp\n@@ -1437,7 +1437,9 @@ namespace {\n \n     Value ExpressionMillisecond::evaluateInternal(const Variables& vars) const {\n         Value date(vpOperand[0]->evaluateInternal(vars));\n-        return Value(extractMillisPortion(date.coerceToDate()));\n+        const int ms = date.coerceToDate() % 1000LL;\n+        // adding 1000 since dates before 1970 would have negative ms\n+        return Value(ms >= 0 ? ms : 1000 + ms);\n     }\n \n     REGISTER_EXPRESSION(\"$millisecond\", ExpressionMillisecond::parse);\ndiff --git a/src/mongo/db/pipeline/value.cpp b/src/mongo/db/pipeline/value.cpp\nindex 26f01e8..9139f0c 100644\n--- a/src/mongo/db/pipeline/value.cpp\n+++ b/src/mongo/db/pipeline/value.cpp\n@@ -426,7 +426,22 @@ namespace mongo {\n     }\n \n     time_t Value::coerceToTimeT() const {\n-        return millisToTimeT(coerceToDate());\n+        long long millis = coerceToDate();\n+        if (millis < 0) {\n+            // We want the division below to truncate toward -inf rather than 0\n+            // eg Dec 31, 1969 23:59:58.001 should be -2 seconds rather than -1\n+            // This is needed to get the correct values from coerceToTM\n+            if ( -1999 / 1000 != -2) { // this is implementation defined\n+                millis -= 1000-1;\n+            }\n+        }\n+        const long long seconds = millis / 1000;\n+\n+        uassert(16421, \"Can't handle date values outside of time_t range\",\n+               seconds >= std::numeric_limits<time_t>::min() &&\n+               seconds <= std::numeric_limits<time_t>::max());\n+\n+        return static_cast<time_t>(seconds);\n     }\n     tm Value::coerceToTm() const {\n         // See implementation in Date_t.\ndiff --git a/src/mongo/util/time_support.cpp b/src/mongo/util/time_support.cpp\nindex fc7b71e..7619050 100644\n--- a/src/mongo/util/time_support.cpp\n+++ b/src/mongo/util/time_support.cpp\n@@ -112,13 +112,12 @@ namespace mongo {\n         const int bufSize = 32;\n         char buf[bufSize];\n         struct tm t;\n-        time_t_to_Struct(millisToTimeT(static_cast<long long>(date.millis)), &t, local);\n+        time_t_to_Struct(date.toTimeT(), &t, local);\n         int pos = strftime(buf, bufSize, MONGO_ISO_DATE_FMT_NO_TZ, &t);\n         fassert(16981, 0 < pos);\n         char* cur = buf + pos;\n         int bufRemaining = bufSize - pos;\n-        pos = snprintf(cur, bufRemaining, \".%03d\",\n-                       extractMillisPortion(static_cast<long long>(date.millis)));\n+        pos = snprintf(cur, bufRemaining, \".%03d\", static_cast<int32_t>(date.asInt64() % 1000));\n         fassert(16982, bufRemaining > pos && pos > 0);\n         cur += pos;\n         bufRemaining -= pos;\n@@ -183,30 +182,6 @@ namespace mongo {\n         return millis / 1000;\n     }\n \n-    time_t millisToTimeT(long long millis) {\n-        if (millis < 0) {\n-            // We want the division below to truncate toward -inf rather than 0\n-            // eg Dec 31, 1969 23:59:58.001 should be -2 seconds rather than -1\n-            // This is needed to get the correct values from coerceToTM\n-            if ( -1999 / 1000 != -2) { // this is implementation defined\n-                millis -= 1000-1;\n-            }\n-        }\n-        const long long seconds = millis / 1000;\n-\n-        uassert(16421, \"Can't handle date values outside of time_t range\",\n-               seconds >= std::numeric_limits<time_t>::min() &&\n-               seconds <= std::numeric_limits<time_t>::max());\n-\n-        return static_cast<time_t>(seconds);\n-    }\n-\n-    int extractMillisPortion(long long millisSinceEpoch) {\n-        const int ms = millisSinceEpoch % 1000LL;\n-        // adding 1000 since dates before 1970 would have negative ms\n-        return ms >= 0 ? ms : 1000 + ms;\n-    }\n-\n     std::string dateToCtimeString(Date_t date) {\n         time_t t = date.toTimeT();\n         char buf[64];\ndiff --git a/src/mongo/util/time_support.h b/src/mongo/util/time_support.h\nindex 1043a65..46cee85 100644\n--- a/src/mongo/util/time_support.h\n+++ b/src/mongo/util/time_support.h\n@@ -78,18 +78,6 @@ namespace mongo {\n      */\n     std::string dateToCtimeString(Date_t date);\n \n-    /**\n-     * Converts millis to time_t, doing correct division for negative millis, and uasserting that\n-     * the result falls within the valid range of a time_t.\n-     */\n-    time_t millisToTimeT(long long millis);\n-\n-    /**\n-     * Returns the millis since the last whole second of the given millis since epoch, and correctly\n-     * handles dates before epoch.\n-     */\n-    int extractMillisPortion(long long millisSinceEpoch);\n-\n     boost::gregorian::date currentDate();\n \n     // parses time of day in \"hh:mm\" format assuming 'hh' is 00-23\ndiff --git a/src/mongo/util/time_support_test.cpp b/src/mongo/util/time_support_test.cpp\nindex 12a72d4..c459c12 100644\n--- a/src/mongo/util/time_support_test.cpp\n+++ b/src/mongo/util/time_support_test.cpp\n@@ -63,53 +63,6 @@ namespace {\n                           dateToISOStringUTC(Date_t(2781455351100ULL)));\n         ASSERT_EQUALS(std::string(\"2013-02-20T18:29:11.100Z\"),\n                       dateToISOStringUTC(Date_t(1361384951100ULL)));\n-\n-        // Basic test\n-#ifndef _WIN32 // Negative Dates don't currently work on Windows\n-        ASSERT_EQUALS(std::string(\"1960-01-02T03:04:05.006Z\"),\n-                      dateToISOStringUTC(Date_t(-315521754994LL)));\n-#endif\n-\n-        // Testing special rounding rules for seconds\n-#ifndef _WIN32 // Negative Dates don't currently work on Windows\n-        ASSERT_EQUALS(std::string(\"1960-01-02T03:04:04.999Z\"),\n-                      dateToISOStringUTC(Date_t(-315521755001LL))); // second = 4\n-        ASSERT_EQUALS(std::string(\"1960-01-02T03:04:05.000Z\"),\n-                      dateToISOStringUTC(Date_t(-315521755000LL))); // second = 5\n-        ASSERT_EQUALS(std::string(\"1960-01-02T03:04:05.001Z\"),\n-                      dateToISOStringUTC(Date_t(-315521754999LL))); // second = 5\n-        ASSERT_EQUALS(std::string(\"1960-01-02T03:04:05.999Z\"),\n-                      dateToISOStringUTC(Date_t(-315521754001LL))); // second = 5\n-#endif\n-\n-        // Test date before 1900 (negative tm_year values from gmtime)\n-#ifndef _WIN32 // Negative Dates don't currently work on Windows\n-        if (!isTimeTSmall)\n-            ASSERT_EQUALS(std::string(\"1860-01-02T03:04:05.006Z\"),\n-                          dateToISOStringUTC(Date_t(-3471195354994LL)));\n-#endif\n-\n-        // Test with time_t == -1\n-#ifndef _WIN32 // Negative Dates don't currently work on Windows\n-        ASSERT_EQUALS(std::string(\"1969-12-31T23:59:59.000Z\"),\n-                      dateToISOStringUTC(Date_t(-1000LL)));\n-#endif\n-\n-        // Testing dates between 1970 and 2000\n-        ASSERT_EQUALS(std::string(\"1970-01-01T00:00:00.000Z\"),\n-                      dateToISOStringUTC(Date_t(0ULL)));\n-        ASSERT_EQUALS(std::string(\"1970-01-01T00:00:00.999Z\"),\n-                      dateToISOStringUTC(Date_t(999ULL)));\n-        ASSERT_EQUALS(std::string(\"1980-05-20T12:54:04.834Z\"),\n-                      dateToISOStringUTC(Date_t(327675244834ULL)));\n-        ASSERT_EQUALS(std::string(\"1999-12-31T00:00:00.000Z\"),\n-                      dateToISOStringUTC(Date_t(946598400000ULL)));\n-        ASSERT_EQUALS(std::string(\"1999-12-31T23:59:59.999Z\"),\n-                      dateToISOStringUTC(Date_t(946684799999ULL)));\n-\n-        // Test date > 2000 for completeness (using now)\n-        ASSERT_EQUALS(std::string(\"2013-10-11T23:20:12.072Z\"),\n-                      dateToISOStringUTC(Date_t(1381533612072ULL)));\n     }\n \n     TEST(TimeFormatting, DateAsISO8601Local) {\n","summary":[{"filename":"jstests/tool/csvexport_dates.js","additions":0,"deletions":77},{"filename":"src/mongo/db/pipeline/expression.cpp","additions":3,"deletions":1},{"filename":"src/mongo/db/pipeline/value.cpp","additions":16,"deletions":1},{"filename":"src/mongo/util/time_support.cpp","additions":2,"deletions":27},{"filename":"src/mongo/util/time_support.h","additions":0,"deletions":12},{"filename":"src/mongo/util/time_support_test.cpp","additions":0,"deletions":47}]}}],"tasks":[],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"52691f903ff1223136000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["cxx11-ubuntu1204-64","linux-64","osx-108-cxx11-debug","osx-108-debug","rhel-57-64-bit","solaris-64-bit","windows-32","windows-64-2k8"],"create_time":{"$date":"2013-10-24T13:24:32.386Z"},"desc":"SERVER-11333 test run 4","githash":"d431dc6ca4b15bed750463fccd334d3b681da73e","patches":[{"name":"","githash":"d431dc6ca4b15bed750463fccd334d3b681da73e","patch_set":{"patch":"diff --git a/SConstruct b/SConstruct\nindex 5f33a58..4d768dc 100644\n--- a/SConstruct\n+++ b/SConstruct\n@@ -201,6 +201,7 @@ add_option( \"static\" , \"fully static build\" , 0 , False )\n add_option( \"static-libstdc++\" , \"statically link libstdc++\" , 0 , False )\n add_option( \"lto\", \"enable link time optimizations (experimental, except with MSVC)\" , 0 , True )\n add_option( \"dynamic-windows\", \"dynamically link on Windows\", 0, True)\n+add_option( \"disable-declspec-thread\", \"don't use __declspec(thread) on Windows\", 0, True)\n \n # base compile flags\n add_option( \"64\" , \"whether to force 64 bit\" , 0 , True , \"force64\" )\n@@ -1321,6 +1322,51 @@ def doConfigure(myenv):\n     if linux:\n         AddToCCFLAGSIfSupported(myenv, \"-fno-builtin-memcmp\")\n \n+    # When using msvc, check for support for __declspec(thread), unless we have been asked\n+    # explicitly not to use it. For other compilers, see if __thread works.\n+    if using_msvc():\n+        haveDeclSpecThread = False\n+        if not has_option(\"disable-declspec-thread\"):\n+            def CheckDeclspecThread(context):\n+                test_body = \"\"\"\n+                __declspec( thread ) int tsp_int;\n+                int main(int argc, char* argv[]) {\n+                    tsp_int = argc;\n+                    return 0;\n+                }\n+                \"\"\"\n+                context.Message('Checking for __declspec(thread)... ')\n+                ret = context.TryLink(textwrap.dedent(test_body), \".cpp\")\n+                context.Result(ret)\n+                return ret\n+            conf = Configure(myenv, help=False, custom_tests = {\n+                'CheckDeclspecThread' : CheckDeclspecThread,\n+            })\n+            haveDeclSpecThread = conf.CheckDeclspecThread()\n+            conf.Finish()\n+        if haveDeclSpecThread:\n+            myenv.Append(CPPDEFINES=['MONGO_HAVE___DECLSPEC_THREAD'])\n+    else:\n+        def CheckUUThread(context):\n+            test_body = \"\"\"\n+            __thread int tsp_int;\n+            int main(int argc, char* argv[]) {\n+                tsp_int = argc;\n+                return 0;\n+            }\n+            \"\"\"\n+            context.Message('Checking for __thread... ')\n+            ret = context.TryLink(textwrap.dedent(test_body), \".cpp\")\n+            context.Result(ret)\n+            return ret\n+        conf = Configure(myenv, help=False, custom_tests = {\n+            'CheckUUThread' : CheckUUThread,\n+        })\n+        haveUUThread = conf.CheckUUThread()\n+        conf.Finish()\n+        if haveUUThread:\n+            myenv.Append(CPPDEFINES=['MONGO_HAVE___THREAD'])\n+\n     conf = Configure(myenv)\n     libdeps.setup_conftests(conf)\n \ndiff --git a/src/mongo/db/storage/record.cpp b/src/mongo/db/storage/record.cpp\nindex 9f2b354..46e3988 100644\n--- a/src/mongo/db/storage/record.cpp\n+++ b/src/mongo/db/storage/record.cpp\n@@ -395,12 +395,12 @@ namespace mongo {\n \n     \n     // These need to be outside the ps namespace due to the way they are defined\n-#if defined(__linux__) && defined(__GNUC__)\n+#if defined(MONGO_HAVE___THREAD)\n     __thread ps::PointerTable::Data _pointerTableData;\n     ps::PointerTable::Data* ps::PointerTable::getData() { \n         return &_pointerTableData; \n     }\n-#elif defined(_WIN32)\n+#elif defined(MONGO_HAVE___DECLSPEC_THREAD)\n     __declspec( thread ) ps::PointerTable::Data _pointerTableData;\n     ps::PointerTable::Data* ps::PointerTable::getData() { \n         return &_pointerTableData; \ndiff --git a/src/mongo/util/concurrency/threadlocal.h b/src/mongo/util/concurrency/threadlocal.h\nindex 05109ad..a9b0b42 100644\n--- a/src/mongo/util/concurrency/threadlocal.h\n+++ b/src/mongo/util/concurrency/threadlocal.h\n@@ -86,7 +86,7 @@ namespace mongo {\n        a combination here, with the assumption that reset's are infrequent, so that \n        get's are fast.\n     */\n-#if defined(_WIN32) || (defined(__GNUC__) && defined(__linux__))\n+#if defined(MONGO_HAVE___THREAD) || defined(MONGO_HAVE___DECLSPEC_THREAD)\n         \n     template< class T >\n     struct TSP {\n@@ -102,7 +102,7 @@ namespace mongo {\n         }\n     };\n \n-# if defined(_WIN32)\n+# if defined(MONGO_HAVE___DECLSPEC_THREAD)\n \n #  define TSP_DECLARE(T,p) extern TSP<T> p;\n \n@@ -129,7 +129,7 @@ namespace mongo {\n     TSP<T> p;\n # endif\n \n-#elif defined(__APPLE__)\n+#elif defined(_POSIX_THREADS) && (_POSIX_THREADS >= 0)\n     template< class T>\n     struct TSP {\n         pthread_key_t _key;\n","summary":[{"filename":"SConstruct","additions":46,"deletions":0},{"filename":"src/mongo/db/storage/record.cpp","additions":2,"deletions":2},{"filename":"src/mongo/util/concurrency/threadlocal.h","additions":3,"deletions":3}]}}],"tasks":["core","client","js","durability","sharding","aggregation"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"526983de3ff1224641000001"},"activated":true,"author":"admin","branch":"mongodb-mongo-master","project_storage_method":"db","build_variants":["cxx11-ubuntu1204-64","enterprise-linux-64-amazon-ami","enterprise-rhel-57-64-bit","enterprise-rhel-62-64-bit","enterprise-suse11-64","enterprise-ubuntu1204-64","enterprise-windows-64","linux-32","linux-32-debug","linux-64","linux-64-debug","linux-64-debug-duroff","linux-64-duroff","osx-108","osx-108-cxx11-debug","osx-108-debug","osx-108-dur-off","rhel-57-64-bit","solaris-64-bit","windows-32","windows-64","windows-64-2k8","windows-64-2k8-debug","windows-64-debug"],"create_time":{"$date":"2013-10-24T20:32:30.842Z"},"desc":"the right version of ssl_fips","githash":"bc05262b903baafd4a286008ab3323172d8c3275","patches":[{"name":"","githash":"bc05262b903baafd4a286008ab3323172d8c3275","patch_set":{"patch":"diff --git a/jstests/ssl/ssl_fips.js b/jstests/ssl/ssl_fips.js\nindex 76118a7..7b2da1a 100644\n--- a/jstests/ssl/ssl_fips.js\n+++ b/jstests/ssl/ssl_fips.js\n@@ -1,5 +1,4 @@\n // Test mongod start with FIPS mode enabled\n-if (0) { // SERVER-11005\n ports = allocatePorts(1);\n port1 = ports[0];\n var baseName = \"jstests_ssl_ssl_fips\";\n@@ -10,11 +9,21 @@ var md = startMongod(\"--port\", port1, \"--dbpath\",\n                      \"--sslPEMKeyFile\", \"jstests/libs/server.pem\",\n                      \"--sslFIPSMode\");\n \n-var mongo = runMongoProgram(\"mongo\", \"--port\", port1, \"--ssl\",\n-                            \"--sslPEMKeyFile\", \"jstests/libs/client.pem\",\n-                            \"--sslFIPSMode\",\n-                            \"--eval\", \";\");\n+// if mongod didn't start properly\n+if (md != 0) {\n+    print(\"mongod failed to start, checking for FIPS support\");\n+    assert(rawMongoProgramOutput().match(\n+            /this version of mongodb was not compiled with FIPS support/));\n+}\n+else {\n+    // try connecting shell\n+    var mongo = runMongoProgram(\"mongo\", \"--port\", port1, \"--ssl\",\n+                                \"--sslPEMKeyFile\", \"jstests/libs/client.pem\",\n+                                \"--sslFIPSMode\",\n+                                \"--eval\", \";\");\n \n-// 0 is the exit code for success\n-assert(mongo==0);\n+    // 0 is the exit code for success\n+    assert(mongo==0);\n+    // kill mongod\n+    stopMongod(port1, 9);\n }\n","summary":[{"filename":"jstests/ssl/ssl_fips.js","additions":16,"deletions":7}]}}],"tasks":["ssl"],"version":"5273b4b83ff1226c75000003_0","status":"started"}
{"_id":{"$oid":"5f74d99ab2373627c047c5e5"},"triggers":{"child_patches":["5ecedafb562343215a7ff297","5e4ff3abe3c3317e352062e4","52420b363ff1222d23000002"]},"desc":"main: EVG-7823 add a commit queue message (#4048)","branch":"evergreen","githash":"a1d2c8f70bf5c543de8b9641ac1ec08def1ddb26","patch_number":234,"author":"admin","version":"5f74d99ab2373627c047c5e5","status":"success","create_time":{"$date":"2020-09-30T19:16:42.44Z"},"start_time":{"$date":"2020-09-30T21:30:05.654Z"},"finish_time":{"$date":"2020-09-30T21:32:16.13Z"},"project_storage_method":"db","build_variants":["ubuntu1604"],"tasks":["test-command","test-util","test-db"],"variants_tasks":[{"variant":"ubuntu1604","tasks":["test-command","test-util","test-db"],"displaytasks":[{"name":"asdf"}]}],"sync_at_end_opts":{"timeout":3600000000000},"patches":[{"name":"","githash":"a1d2c8f70bf5c543de8b9641ac1ec08def1ddb26","patch_set":{"patch_file_id":"5e4ff3ab850e6136624eaf95","commit_messages":["main: EVG-7823 add a commit queue message (#4048)"],"summary":[{"filename":"self-tests.yml","additions":6,"deletions":0,"description":"main: EVG-7823 add a commit queue message (#4048)"}]},"is_mbox":true}],"parameters":[{"key":"my_param","value":"my_value"}],"activated":true,"alias":"","merge_patch":""}
{"_id":{"$oid":"5e6bb9e23066155a993e0f1b"},"desc":"Patch with display tasks","branch":"evergreen","githash":"25ab18d7ed2775f27be77d8135ddd841c78cfe28","patch_number":452,"author":"admin","version":"","status":"created","create_time":{"$date":"2020-03-13T16:50:42.981Z"},"start_time":{"$date":{"$numberLong":"-62135596800000"}},"finish_time":{"$date":{"$numberLong":"-62135596800000"}},"project_storage_method":"db","build_variants":["ubuntu1604"],"tasks":["test-graphql"],"variants_tasks":[{"variant":"ubuntu1604","tasks":["test-graphql"],"displaytasks":[]}],"patches":[],"activated":false,"alias":""}
